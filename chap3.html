<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="style.css" type="text/css" />
</head>
<body>
<nav>
    <a href="chap2.html" class="navigation previous">Anterior</a>
      <a href="chap4.html" class="navigation next">Siguiente</a>
  </nav>
<h1 id="parsing-ascendente-bottom-up">Parsing Ascendente (<em>Bottom-Up</em>)</h1>
<p>En la sección anterior vimos las técnicas de parsing descendente, y aprendimos algunas de las limitaciones más importantes que tienen. Dado que el objetivo es poder predecir exactamente que producción es necesario ejecutar en cada momento, las gramáticas LL(1) tienen fuertes restricciones. En particular, deben estar factorizadas, y no pueden tener recursión izquierda (criterios que son necesarios pero no suficientes). Por este motivo, para convertir una gramática &quot;natural&quot; en una gramática LL(1) es necesario adicionar no-terminales para factorizar y eliminar la recursión, que luego no tienen ningún significado semántico. Los árboles de derivación de estas gramáticas son por tanto más complejos, y tienen menos relación con el árbol de sintaxis abstracta que queremos obtener finalmente (aunque aún no hemos definido este concepto formalmente).</p>
<p>Intuitivamente, el problema con los parsers descendentes, es que son demasiado exigentes. En cada momento, se quiere saber qué producción hay que aplicar para obtener la porción de cadena que sigue. En otras palabras, a partir de una forma oracional, tenemos que decidir cómo expandir el no-terminal más a la izquierda, de modo que la siguiente forma oracional esté &quot;más cerca&quot; de generar la cadena. Por este motivo se llama parsing descendente.</p>
<p>¿Qué pasa si pensamos el problema de forma inversa? Comenzamos con la cadena completa, y vamos a intentar reducir fragmentos de la cadena, aplicando producciones &quot;a la inversa&quot; hasta lograr reducir toda la cadena a <code>S</code>. En vez de intentar adivinar que producción aplicar &quot;de ahora en adelante&quot;, intentaremos deducir, dado un prefijo de la cadena analizado, qué producción se puede &quot;desaplicar&quot; para reducir ese prefijo a una forma oracional que esté &quot;más cerca&quot; del símbolo inicial. Si pensamos el problema de forma inversa, puede que encontremos una estrategia de parsing que sea más permisiva con las gramáticas.</p>
<p>Veamos un ejemplo. Recordemos la gramática &quot;natural&quot; no ambigua para expresiones aritméticas:</p>
<pre><code>E = T + E | T
T = int * T | int | ( E )</code></pre>
<p>Y la cadena de siempre: <code>int * ( int + int )</code>. Tratemos ahora de construir una derivación &quot;de abajo hacia arriba&quot;, tratando de reducir esta cadena al símbolo inicial <code>E</code> aplicando producciones a la inversa. Vamos a representar con una barra vertical <code>|</code> el punto que divide el fragmento de cadena que hemos analizado del resto. De modo que empezamos por:</p>
<pre><code>|int * ( int + int )</code></pre>
<p>Miramos entonces el primer token:</p>
<pre><code> int|* ( int + int )</code></pre>
<p>El primer token de la cadena es <code>int</code>, que se puede reducir aplicando <code>T -&gt; int</code> a la inversa. Sin embargo, esta reducción no es conveniente. ¿Por qué? El problema es que queremos lograr reducir hasta <code>E</code>, por tanto hay que tener un poco de &quot;luz larga&quot; y aplicar reducciones que, en principio, dejen la posibilidad de seguir reduciendo hasta <code>E</code>. Como no existe ninguna producción que derive en <code>T * w</code>, si reducimos <code>T -&gt; int</code> ahora no podremos seguir reduciendo en el futuro. Más adelante formalizaremos esta idea. Seguimos entonces buscando hacia la derecha en la cadena:</p>
<pre><code> int *|( int + int )
 int * (|int + int )
 int * ( int|+ int )</code></pre>
<p>En este punto podemos ver que sí es conveniente reducir <code>T -&gt; int</code>, porque luego viene un <code>+</code> y tenemos, en principio, la posibilidad de seguir reduciendo aplicando <code>E -&gt; T + E</code> en el futuro:</p>
<pre><code> int * ( T|+ int )</code></pre>
<p>Avanzamos hacia el siguiente token reducible:</p>
<pre><code> int * ( T +|int )
 int * ( T + int|)</code></pre>
<p>Aquí nuevamente podemos aplicar la reducción <code>T -&gt; int</code>:</p>
<pre><code> int * ( T + T|)</code></pre>
<p>Antes de continuar, dado que tenemos justo delante de la barra (<code>|</code>) un sufijo <code>T + T</code>, deberíamos darnos cuenta que es conveniente reducir <code>E -&gt; T</code> para luego poder reducir <code>E -&gt; T + E</code>:</p>
<pre><code> int * ( T + E|)
 int * ( E|)</code></pre>
<p>En este punto, no hay reducciones evidentes que realizar, así que seguimos avanzando:</p>
<pre><code> int * ( E )|</code></pre>
<p>Hemos encontrado entonces un sufijo <code>( E )</code> que podemos reducir con <code>T -&gt; (E)</code>:</p>
<pre><code> int * T|</code></pre>
<p>Luego reducimos <code>T -&gt; int * T</code>:</p>
<pre><code> T|</code></pre>
<p>Y finalmente reducimos <code>E -&gt; T</code>:</p>
<pre><code> E</code></pre>
<p>En este punto hemos logrado reducir al símbolo inicial toda la cadena. Veamos la secuencia de formas oracionales que hemos obtenido:</p>
<pre><code> int * ( int + int )
 int * ( T + int )
 int * ( T + T )
 int * ( T + E )
 int * ( E )
 int * T
 T
 E</code></pre>
<p>Si observamos esta secuencia en orden inverso, veremos que es una derivación extrema derecha de <code>E -*-&gt; int * ( int + int )</code>. Justamente, un parser ascendente se caracteriza porque construye una derivación extrema derecha en orden inverso, desde la cadena hacia el símbolo inicial. Tratemos ahora de formalizar este proceso. Para ello, notemos primero algunas propiedades interesante que cumple todo parser ascendente. Partimos del hecho que hemos dado como definición de un parser bottom-up:</p>
<blockquote>
<p>Un parser bottom-up construye una derivación extrema derecha de <span class="math inline"><em>S</em>→<sup>*</sup><em>ω</em></span>.</p>
</blockquote>
<p>A partir de este hecho, que hemos dado como definición, podemos deducir una consecuencia muy interesante:</p>
<blockquote>
<p>Sea <span class="math inline"><em>α</em><em>β</em><em>ω</em></span> una forma oracional en un paso intermedio de un parser ascendente. Sea <span class="math inline"><em>X</em> → <em>β</em></span> la siguiente reducción a realizar. Entonces <span class="math inline"><em>ω</em></span> es una cadena de terminales (formalmente <span class="math inline"><em>ω</em> ∈ <em>T</em><sup>*</sup></span>).</p>
</blockquote>
<p>Para ver por qué esto es cierto, basta notar que si la derivación que construiremos es extrema derecha, la aplicación de <span class="math inline"><em>X</em> → <em>β</em></span> en este paso solamente puede ocurrir si <span class="math inline"><em>X</em></span> es el no-terminal más a la derecha. O sea, si <span class="math inline"><em>α</em><em>β</em><em>ω</em></span> es el paso correspondiente, y reducimos por <span class="math inline"><em>X</em> → <em>β</em></span>, entonces el siguiente paso es la forma oracional <span class="math inline"><em>α</em><em>X</em><em>ω</em></span>, donde <span class="math inline"><em>X</em></span> es el no-terminal más a la derecha, debido justamente a que estamos construyendo una derivación extrema derecha.</p>
<p>Esta propiedad nos permite entonces entender que en todo paso de un parser ascendente, cada vez que sea conveniente reducir <span class="math inline"><em>X</em> → <em>β</em></span>, es porque existe una posición (que hemos marcado con <code>|</code>), tal que <span class="math inline"><em>α</em><em>β</em>|<em>ω</em></span> es la forma oracional, donde <span class="math inline"><em>α</em><em>β</em> ∈ {<em>N</em> ∪ <em>T</em>}<sup>*</sup></span> y <span class="math inline"><em>ω</em> ∈ <em>T</em><sup>*</sup></span>. Tenemos entonces dos tipos de operaciones que podemos realizar, que llamaremos <strong>shift</strong> y <strong>reduce</strong>. La operación <strong>shift</strong> nos permite mover la barra <code>|</code> un token hacia la derecha, lo que equivale a decir que analizamos el siguiente token. La operación <strong>reduce</strong> nos permite coger un sufijo de la forma oracional que está antes de la barra <code>|</code> y reducirla a un no-terminal, aplicando una producción a la inversa (o &quot;desaplicando&quot; la producción). Veamos nuevamente la secuencia de operaciones que hemos realizado, notando las que fueron <strong>shift</strong> y las que fueron <strong>reduce</strong>:</p>
<pre><code>|int * ( int + int )         | shift
 int|* ( int + int )         | shift
 int *|( int + int )         | shift
 int * (|int + int )         | shift
 int * ( int|+ int )         | reduce T -&gt; int
 int * ( T|+ int )           | shift
 int * ( T +|int )           | shift
 int * ( T + int|)           | reduce T -&gt; int
 int * ( T + T|)             | reduce E -&gt; T
 int * ( T + E|)             | reduce E -&gt; T + E
 int * ( E|)                 | shift
 int * ( E )|                | reduce T -&gt; ( E )
 int * T|                    | reduce T -&gt; int * T
 T|                          | reduce E -&gt; T
 E                           | OK</code></pre>
<p>Debido a estas operaciones, llamaremos a este tipo de mecanismos <em>parsers shift-reduce</em>. Veamos de forma general como implementar este tipo de parsers.</p>
<h2 id="parsers-shift-reduce">Parsers <em>Shift-Reduce</em></h2>
<p>Notemos que la parte a la izquierda de la barra siempre cambia porque un sufijo es parte derecha de una producción, y se reduce a un no-terminal. La parte derecha solo cambia cuando un terminal &quot;cruza&quot; la barra y se convierte en parte del sufijo que será reducido en el futuro. De forma que la barra que la parte izquierda se comporta como una pila, ya que solamente se introducen terminales por un extremo, y se extraen símbolos (terminales o no-terminales) por el mismo extremo. La parte derecha es simplemente una secuencia de tokens, que se introducen en la pila uno a uno. Formalicemos entonces el funcionamiento de un parser <em>shift-reduce</em>.</p>
<p>Un parser <em>shift-reduce</em> es un mecanismo de parsing que cuenta con las siguientes estructuras:</p>
<ul>
<li>Una pila de símbolos <code>S</code>.</li>
<li>Una secuencia de terminales <code>T</code>.</li>
</ul>
<p>Y las operaciones siguientes:</p>
<ul>
<li><strong>shift</strong>: Si <span class="math inline"><em>S</em> = <em>α</em>|</span> es el contenido de la pila, y <span class="math inline"><em>T</em> = <em>c</em><em>ω</em>$</span> la secuencia de terminales, entonces tras aplicar una operación <strong>shift</strong> se tiene en la pila <span class="math inline"><em>S</em>′=<em>α</em><em>c</em>|</span>, y la secuencia de terminales ahora es <span class="math inline"><em>T</em>′=<em>ω</em>$</span>. Es decir, se mete en la pila el token <span class="math inline"><em>c</em></span>.</li>
<li><strong>reduce</strong>: Si <span class="math inline"><em>S</em> = <em>α</em><em>β</em>|</span> el contenido de la pila, y <span class="math inline"><em>X</em> → <em>β</em></span> es una producción, entonces tras aplicar una operación <strong>reduce <span class="math inline"><em>T</em> → <em>β</em></span></strong> se tiene en la pila <span class="math inline"><em>S</em>′=<em>α</em><em>X</em>|</span>. La secuencia de terminales no se modifica. Es decir, se extraen de la pila <span class="math inline">|<em>β</em>|</span> símbolos y se introduce el símbolo <span class="math inline"><em>X</em></span> correspondiente.</li>
</ul>
<p>Podemos definir entonces el proceso de parsing como:</p>
<blockquote>
<p>Sea <span class="math inline"><em>S</em> = ∅</span> la pila inicial, <span class="math inline"><em>T</em> = <em>ω</em>$</span> la cadena a reconocer, y <span class="math inline"><em>E</em></span> el símbolo inicial, un parser shift-reduce reconoce esta cadena si y solo si existe una secuencia de operaciones <strong>shift</strong> y <strong>reduce</strong> tal que tras aplicarlas se obtiene <span class="math inline"><em>S</em> = <em>E</em></span> y <span class="math inline"><em>T</em> = $</span>.</p>
</blockquote>
<p>Es decir, un parser shitf-reduce básicamente tiene que aplicar operaciones <em>convenientemente</em> hasta que en la pila solamente quede el símbolo inicial, y se hayan consumido todos los tokens de la cadena de entrada. En este punto, se ha logrado construir una derivación extrema derecha de la cadena correspondiente. Por supuesto, existe un grado importante de no determinismo en esta definición, porque en principio puede haber muchas secuencias de operaciones shift-reduce que permitan llegar al símbolo inicial. Si asumimos que la gramática no es ambigua, y por tanto solo existe una derivación extrema derecha, podemos intuir que debe ser posible construir un parser que encuentre la secuencia de shift-reduce que produce esa derivación. Desgraciadamente esto no es posible para todo tipo de gramáticas libre del contexto, pero existen gramáticas más restringidas para las que sí es posible decidir de forma determinista en todo momento si la operación correcta es <strong>shift</strong> o <strong>reduce</strong>, y en el segundo caso a qué símbolo reducir.</p>
<p>Para simplificar la notación, en ocasiones identificaremos el estado de un parser shift-reduce en la forma <span class="math inline"><em>α</em>|<em>ω</em></span>, sobreentendiendo que el estado de la pila es <span class="math inline"><em>S</em> = <em>α</em>|</span> y la cadena de entrada es <span class="math inline"><em>ω</em>$</span>. Diremos además que un estado <span class="math inline"><em>α</em>|<em>ω</em></span> es válido, si y solo si la cadena pertenece al lenguaje, y este estado forma parte de los estados necesarios para completar el parsing de forma correcta.</p>
<p>Este tipo de parsers son en la práctica los más usados, pues permiten reconocer una cadena (y construir la derivación) con un costo lineal en la longitud de la cadena (la misma eficiencia que los parsers LL), y permiten parsear gramáticas mucho más poderosas y expresivas que las gramáticas LL. De hecho, la mayoría de los compiladores modernos usan alguna variante de un parser shift-reduce. La diferencia entre todos ellos radica justamente en cómo se decide en cada paso qué operación aplicar. Formalicemos entonces el problema de decisión planteado. Tomemos de nuevo la gramática anterior, y recordemos que en el paso:</p>
<pre><code>int|* ( int + int )</code></pre>
<p>Habíamos dicho que aunque era posible reducir <code>T -&gt; int</code>, no era convieniente hacerlo, porque caeríamos en una forma oracional que no puede ser reducida a <code>E</code>. En particular, en este caso caeríamos en:</p>
<pre><code>T|* ( int + int )</code></pre>
<p>Y sabemos intuitivamente que esta forma oracional no es reducible a <code>E</code>, porque no existe ninguna producción que comience por <code>T *</code>, o dicho de otra forma, <code>*</code> no pertenece al <code>Follow(T)</code>. Tratemos de formalizar entonces este concepto de &quot;momento donde es conveniente reducir&quot;. Para ello introduciremos una definición que formaliza esta intuición.</p>
<blockquote>
<p>Sea <span class="math inline"><em>S</em>→<sup>*</sup><em>α</em><em>X</em><em>ω</em> → <em>α</em><em>β</em><em>ω</em></span> una derivación extrema derecha de la forma oracional <span class="math inline"><em>α</em><em>β</em><em>ω</em></span>, y <span class="math inline"><em>X</em> → <em>β</em></span> una producción, decimos que <span class="math inline"><em>α</em><em>β</em></span> es un <strong>handle</strong> de <span class="math inline"><em>α</em><em>β</em><em>ω</em></span>.</p>
</blockquote>
<p>Intuitivamente, un <strong>handle</strong> nos representa un estado en la pila donde es conveniente reducir, porque sabemos que existen reducciones futuras que nos permiten llegar al símbolo inicial. En la definición anterior la pila sería justamente <span class="math inline"><em>α</em><em>β</em>|</span>, y la cadena de terminales sería <span class="math inline"><em>ω</em>$</span>. Sabemos que es posible seguir reduciendo, justamente porque hemos definido un <strong>handle</strong> a partir de conocer que existe una derivación extrema derecha donde aparece ese prefijo. De modo que justamente lo que queremos es identificar cuando tenemos un <strong>handle</strong> en la pila, y en ese momento sabemos que es conveniente reducir.</p>
<p>El problema que nos queda es que hemos definido el concepto de <strong>handle</strong> pero no tenemos una forma evidente de reconocerlos. Resulta que, desgraciadamente no se conoce ningún algoritmo para identificar un <strong>handle</strong> unívocamente en cualquier gramática libre del contexto. Sin embargo, existen algunas heurísticas que nos permiten reconocer algunos <strong>handle</strong> en ciertas ocasiones, y afortunadamente existen gramáticas donde estas heurísticas son suficientes para reconocer todos los <strong>handle</strong> de forma determinista. En última instancia, la diferencia real entre todos los parsers shitf-reduce radica en la estrategia que usen para reconocer los <strong>handle</strong>. Comenzaremos por la más simple.</p>
<h2 id="reconociendo-handles">Reconociendo <strong>Handles</strong></h2>
<p>La forma en la que hemos definido el concepto de <strong>handle</strong> nos permite demostrar una propiedad interesante:</p>
<blockquote>
<p>En un parser shift-reduce, los <strong>handles</strong> aparecen solo en el tope de la pila, nunca en su interior.</p>
</blockquote>
<p>Podemos esbozar una idea de demostración a partir de una inducción fuerte en la cantidad de operaciones <strong>reduce</strong> realizadas. Al inicio, la pila está vacía, y por tanto la hipótesis es trivialmente cierta. Tomemos entonces un estado intermedio de la pila <span class="math inline"><em>α</em><em>β</em>|</span> que es un <strong>handle</strong>. Además, es el único <strong>handle</strong> por hipótesis de inducción fuerte, ya que de lo contrario tendríamos un <strong>handle</strong> en el interior de la pila. Al reducir, el no-terminal más a la derecha queda en el tope de la pila, ya que es una derivación extrema derecha. Por tanto tendremos un nuevo estado en la pila <span class="math inline"><em>α</em><em>X</em>|</span>. Ahora pueden suceder 2 cosas, o bien este estado es un <strong>handle</strong> también (y se cumple la hipótesis), o en caso contrario el siguiente <strong>handle</strong> aparecerá tras alguna secuencia solamente de operaciones <strong>shift</strong>. Este nuevo <strong>handle</strong> tiene que aparecer también en el tope de la pila, pues si apareciera en el interior de la pila, tendría que haber estado antes de <span class="math inline"><em>X</em></span> (lo que es falso por hipótesis de inducción), o tendría que haber aparecido antes del último terminal al que se le hizo <strong>shift</strong>, pero en tal caso deberíamos haber hecho <strong>reduce</strong> en ese <strong>handle</strong>, lo que contradice el hecho de que solo han sucedido operaciones <strong>shift</strong> desde el último <strong>reduce</strong>.</p>
<p>Este teorema nos permite, en primer lugar, formalizar la intuición de que solamente hacen falta movimientos <strong>shift</strong> a la izquierda. Es decir, una vez un terminal ha entrado en la pila, o bien será reducido en algún momento, o bien la cadena es inválida, pero nunca hará falta sacarlo de la pila y volverlo a colocar en la cadena de entrada.</p>
<p>Por otro lado, este teorema nos describe la estructura de la pila, lo que será fundamental para desarrollar un algoritmo de reconocimiento de <strong>handles</strong>. Dado que los <strong>handles</strong> siempre aparecen en el tope de la pila, en todo momento tendremos, en principio, un prefijo de un <strong>handle</strong>. De modo que una idea útil para reconocer <strong>handles</strong> es intentar reconocer cuales son los prefijos de un <strong>handle</strong>. En general, llamaremos <em>prefijo viable</em> a toda forma oracional <span class="math inline"><em>α</em></span> que puede aparecer en la pila durante un reconocimiento válido de una cadena del lenguaje. Formalmente:</p>
<blockquote>
<p>Sea <span class="math inline"><em>α</em>|<em>ω</em></span> un estado válido de un parser shift-reduce durante el reconocimiento de una cadena, entonces decimos que <span class="math inline"><em>α</em></span> es un prefijo viable.</p>
</blockquote>
<p>Intuitivamente, un prefijo viable es un estado en el cual todavía no se ha identificado un error de parsing, por lo que, hasta donde se sabe, la cadena todavía pudiera ser reducida al símbolo inicial. Si podemos reconocer el lenguaje de todos los prefijos viables, en principio siempre sabremos si la pila actual representa un estado válido. Además podemos intuir que esto nos debería ayudar a decidir si hacer un <strong>shift</strong> o un <strong>reduce</strong>, según cual de las dos operaciones nos mantenga el contenido de la pila siendo un prefijo viable. De modo que hemos reducido el problema de identificar <strong>handles</strong> (de forma aproximada) al problema de identificar prefijos viables.</p>
<p>Si analizamos todos los posibles estados válidos de la pila (los posibles prefijos viables), notaremos una propiedad interesante que nos ayudará a reconocer estos prefijos. Supongamos que tenemos un estado <span class="math inline"><em>α</em><em>β</em>|<em>ω</em></span> que es un <strong>handle</strong> para <span class="math inline"><em>X</em> → <em>β</em></span>. Entonces por definición <span class="math inline"><em>α</em><em>β</em></span> es también un prefijo viable. Además, una vez aplicada la reducción, tendremos el estado <span class="math inline"><em>α</em><em>X</em>|<em>ω</em></span>. Por tanto <span class="math inline"><em>α</em><em>X</em></span> también es un prefijo viable, porque de lo contrario esta reducción sería inválida, contradiciendo el hecho de que hemos reducido correctamente en un <strong>handle</strong>. Por tanto o bien <span class="math inline"><em>α</em><em>X</em></span> es un <strong>handle</strong> en sí, o es un prefijo de un <strong>handle</strong>. En el segundo caso, entonces hay una producción <span class="math inline"><em>Y</em> → <em>θ</em><em>X</em><em>ϕ</em></span>, tal que <span class="math inline"><em>α</em> = <em>δ</em><em>θ</em></span>. Es decir, hay un sufjo de <span class="math inline"><em>α</em><em>X</em></span> que tiene que ser prefijo de la parte derecha de esa producción.</p>
<p>¿Por qué?, pues porque como hemos reducido en un <strong>handle</strong>, esto quiere decir que sabemos que es posible en principio seguir reduciendo, por tanto tiene que haber alguna secuencia de tokens <span class="math inline"><em>ϕ</em></span>, que pudiera o no venir en <span class="math inline"><em>ω</em></span> (aún no sabemos), que complete la parte derecha <span class="math inline"><em>θ</em><em>X</em><em>ϕ</em></span>. Es decir, como sabemos que potencialmente podríamos seguir reduciendo, entonces lo que tenemos en la pila ahora tiene que ser prefijo de la parte derecha de alguna producción. Si no lo fuera, ya en este punto podríamos decir que será imposible seguir reduciendo en el futuro, puesto que solamente introduciremos nuevos tokens en la pila, y nunca tocaremos el interior de la pila (excepto a través de reducciones, que siempre modifican el tope de la pila).</p>
<p>Esta intuición nos dice algo muy importante sobre el contenido de la pila:</p>
<blockquote>
<p>En todo estado válido <span class="math inline"><em>α</em>|<em>ω</em></span> de un parser shift-reduce, la forma oracional <span class="math inline"><em>α</em></span> es una secuencia <span class="math inline"><em>α</em> = <em>β</em><sub>1</sub><em>β</em><sub>2</sub>…<em>β</em><sub><em>n</em></sub></span> donde para cada <span class="math inline"><em>β</em><sub><em>i</em></sub></span> se cumple que existe una producción <span class="math inline"><em>X</em> → <em>β</em><sub><em>i</em></sub><em>θ</em></span>.</p>
</blockquote>
<p>Es decir, todo estado válido de la pila es una concatenación de prefijos de partes derechas de alguna producción. En caso contrario, tendríamos una subcadena en la pila que no forma parte de ninguna producción, por tanto no importa lo que pase en el futuro, esta subcadena nunca sería parte de un <strong>reduce</strong>, y por tanto la cadena a reconocer tiene que ser inválida. Más aún, podemos decir exactamente de cuales producciones tienen que ser prefijo esas subcadenas. Dado que en última instancia tenemos que reducir al símbolo inicial <span class="math inline"><em>S</em></span>, entonces en la pila tenemos necesariamente que encontrar prefijos de todas las producciones que participan en la derivación extrema derecha que estamos construyendo. Formalmente:</p>
<blockquote>
<p>Sea <span class="math inline"><em>S</em>→<sup>*</sup><em>α</em><em>δ</em>→<sup>*</sup><em>ω</em></span> la única derivación extrema derecha de <span class="math inline"><em>ω</em></span>, sea <span class="math inline"><em>α</em>|<em>δ</em></span> un estado de un parser shift-reduce que construye esta derivación, sea <span class="math inline"><em>X</em><sub>1</sub> → <em>θ</em><sub>1</sub>, …, <em>X</em><sub><em>n</em></sub> → <em>θ</em><sub><em>n</em></sub></span> la secuencia de producciones a aplicar tal que <span class="math inline"><em>S</em>→<sup>*</sup><em>α</em><em>δ</em></span>, entonces <span class="math inline"><em>α</em> = <em>β</em><sub>1</sub>…<em>β</em><sub><em>n</em></sub></span>, donde <span class="math inline"><em>β</em><sub><em>i</em></sub></span> es prefijo de <span class="math inline"><em>θ</em><sub><em>i</em></sub></span>.</p>
</blockquote>
<p>Es decir, en todo momento en la pila lo que tenemos es una concatenación de prefijos de todas las producciones que quedan por reducir. Notemos intuitivamente que esto debe ser cierto, porque el parser va a construir esta derivación al revés. Por tanto en el estado <span class="math inline"><em>α</em>|<em>δ</em></span>, que corresponde a la forma oracional <span class="math inline"><em>α</em><em>δ</em></span> en la derivación, el parser ya ha reconstruido todas las producciones finales, que hacen que <span class="math inline"><em>α</em><em>δ</em>→<sup>*</sup><em>ω</em></span> (de atrás hacia adelante), y le falta por reconstruir las producciones que hacen que <span class="math inline"><em>S</em>→<sup>*</sup><em>α</em><em>δ</em></span>. Luego, lo que está en la pila tiene que reducirse a <span class="math inline"><em>S</em></span>, y como solo puede pasar que se metan nuevos terminales de <span class="math inline"><em>δ</em></span>, todo lo que está en <span class="math inline"><em>α</em></span> tiene que de algún modo poderse encontrar en alguna de las producciones que faltan por reducir. De lo contrario, esta reducción sería imposible.</p>
<p>Por supuesto, muchos de los prefijos <span class="math inline"><em>β</em><sub><em>i</em></sub></span> pueden ser <span class="math inline"><em>ϵ</em></span>, porque todavía no han aparecido ninguno de los símbolos que forman la producción en la pila (dependen de que reducciones siguientes introduzcan un no-terminal, o de que <strong>shifts</strong> siguientes introduzcan un terminal). De esta forma podemos entender que incluso la pila vacía es una concatenación de prefijos de producciones, todos <span class="math inline"><em>ϵ</em></span>. Lo que no puede pasar es que tengamos una subcadena <span class="math inline"><em>β</em><sub><em>k</em></sub></span> que no forme parte de ningún prefijo de ninguna producción, porque entonces nunca podremos reducir totalmente al símbolo inicial. De modo que un prefijo viable no es nada más que una concatenación de prefijos de las producciones que participan en la derivación extrema derecha que queremos construir.</p>
<p>Para ver un ejemplo tomemos nuevamente nuestra gramática favorita:</p>
<pre><code>E -&gt; T + E | T
T -&gt; int * T | int | (E)</code></pre>
<p>Y veamos la cadena <code>( int )</code>. La derivación extrema derecha que nos genera esta cadena es:</p>
<pre><code>E -&gt; T -&gt; ( E ) -&gt; ( T ) -&gt; ( int )</code></pre>
<p>Para esta cadena <code>( E | )</code> es un estado válido, pues en el siguiente <strong>shift</strong> aparecerá el token <code>)</code> que permite reducir (en dos pasos <code>T -&gt; ( E )</code> y <code>E -&gt; T</code>) al símbolo inicial. Por tanto, <code>( E</code> es un prefijo viable. Veamos cómo este prefijo es una concatenación de prefijos de las dos producciones que faltan por reducir. Evidentemente <code>( E</code> es prefijo de <code>T -&gt; ( E )</code>, y además, <span class="math inline"><em>ϵ</em></span> es prefijo de <code>E -&gt; T</code>.</p>
<p>Esta idea es la pieza fundamental que nos permitirá deducir un algoritmo para reconocer prefijos viables. Como un prefijo viable no es más que una concatenación de prefijos de partes derechas de producciones, simplemente tenemos que reconocer <em>el lenguaje de todas las posibles concatenaciones de prefijos de partes derechas de producciones, que pudieran potencialmente aparecer en una derivación extrema derecha</em>. Parece una definición complicada, pero dado que conocemos la gramática que queremos reconocer, es de hecho bastante fácil. Notemos que hemos dicho, <em>que pudieran aparecer en una derivación</em>, lo cual nos debe dar una idea de cómo construir estas cadenas. Simplemente empezaremos en el símbolo inicial <span class="math inline"><em>S</em></span>, y veremos todas las posibles maneras de derivar, e iremos rastreando los prefijos que se forman. Para esto nos auxiliaremos de un resultado teórico impresionante, que fundamenta toda esta teoría de parsing bottom-up:</p>
<blockquote>
<p>El lenguaje de todos los prefijos viables de una gramática libre del contexto es regular.</p>
</blockquote>
<p>Aunque parece un resultado caído del cielo, de momento podemos comentar lo siguiente. En principio, el lenguaje de todos los posibles prefijos de cada producción es regular (es finito). Y la concatenación de lenguajes regulares es regular. Por tanto, de alguna forma podríamos intuir que este lenguaje de todas las posibles concatenaciones de prefijos debería ser regular. Por tanto debería ser posible construir un autómata finito determinista que lo reconozca. Claro, queda la parte de que no son <em>todas</em> las concatenaciones posibles, sino solo aquellas que aparecen en alguna derivación extrema derecha. Tratemos entonces de construir dicho autómata, y a la vez estaremos con esto demostrando que efectivamente este lenguaje es regular. Recordemos que en última instancia lo que queremos es un autómata que lea el contenido de la pila, y nos diga si es un prefijo viable o no.</p>
<p>Para entender como luce este autómata, introduciremos primero un concepto nuevo. Llamaremos <strong>item</strong> a una cadena de la forma <span class="math inline"><em>X</em> → <em>α</em>.<em>β</em></span>. Es decir, simplemente tomamos una producción y le ponemos un punto (<code>.</code>) en cualquier lugar en su parte derecha. Este <strong>item</strong> formaliza la idea de ver los posibles prefijos de todas las producciones. Por cada producción <span class="math inline"><em>X</em> → <em>δ</em></span>, tenemos <span class="math inline">|<em>δ</em>|+1</span> posibles <strong>items</strong>. Por ejemplo, en la gramática anterior, tenemos los siguientes <strong>items</strong>:</p>
<pre><code>E -&gt; .T + E
E -&gt;  T.+ E
E -&gt;  T +.E
E -&gt;  T + E.

E -&gt; .T
E -&gt;  T.

T -&gt; .int * T
T -&gt;  int.* T
T -&gt;  int *.T
T -&gt;  int * T.

T -&gt; .int
T -&gt;  int.

T -&gt; .( E )
T -&gt;  (.E )
T -&gt;  ( E.)
T -&gt;  ( E ).</code></pre>
<p>Cada uno de estos <strong>items</strong> nos representa un posible prefijo de una producción. Pero además, cada <strong>item</strong> nos permite también rastrear que esperamos ver a continuación de dicha producción, si es que realmente esa producción fuera la que tocara aplicar a continuación. Veamos entonces qué podemos decir de cómo estos <strong>items</strong> se relacionan entre sí. Tomemos por ejemplo el <strong>item</strong> <code>E -&gt; T.+ E</code>. Este <strong>item</strong> nos dice que ya hemos visto algo en la cadena que se reconoce como un <code>T</code>, y que esperamos ver a continuación un <code>+</code>, si resulta que esta es la producción que realmente tocaba aplicar. El <strong>item</strong> <code>E -&gt; .T + E</code> nos dice que si realmente esta es la producción correcta, entonces lo que viene en la cadena debería ser reconocible como un <code>T</code>, y luego debería vernir un <code>+</code>, y luego algo que se reconozca como un <code>E</code>. Por último, un <strong>item</strong> como <code>T -&gt; ( E ).</code> nos dice que ya hemos visto toda la parte derecha de esta producción, y por tanto intuitivamente deberíamos poder reducir. De modo que estos <strong>items</strong> nos están diciendo además si es conviente hacer <strong>shift</strong> o hacer <strong>reduce</strong>.</p>
<p>Vamos a utilizar ahora estos <strong>items</strong> para construir el autómata finito no determinista que nos dirá que es lo que puede venir el pila. Cada estado de este autómata es uno de los <strong>items</strong>. Vamos a decir que el estado asociado asociado al <strong>item</strong> <span class="math inline"><em>X</em> → <em>α</em>.<em>β</em></span> representa que en el tope de la pila tenemos el prefijo <span class="math inline"><em>α</em></span> de esta producción, o en general, algo que es generado por este prefijo <span class="math inline"><em>α</em></span>. Por tanto, todos los estados son estados finales, puesto que cada estado corresponde a un prefijo de alguna producción. Lo que tenemos que hacer es definir entonces un conjunto de transiciones que solamente reconozcan aquellas secuencias de prefijos que consituyen prefijos viables.</p>
<p>Suponamos ahora que tenemos cierto estado de un parser shift-reduce, y queremos saber si es un estado válido. Hagamos a nuestro autómata leer esta la pila desde el fondo hacia el tope, como si fuera una cadena de símbolos. Supongamos entonces que durante esta lectura nos encontramos en cierto estado del autómata, asociado por ejemplo al <strong>item</strong> <code>E -&gt; .T</code>, y hemos leído ya una parte del fondo de la pila, siguiendo las transiciones que aún no hemos definido del todo. La pregunta entonces es qué puede venir a continuación en la pila, justo encima del último símbolo que analizamos. Evidentemente, en la pila podría venir un no-terminal <code>T</code> directamente, que haya aparecido por alguna reducción hecha anteriormente. Si este fuera el caso, entonces todavía tendríamos un prefijo viable. Entonces podemos añadir una transición del estado <code>E -&gt; .T</code> al estado <code>E -&gt; T.</code>.</p>
<p>Por otro lado, incluso si no viniera directamente un <code>T</code>, de todas formas todavía es posible que tengamos un prefijo viable. ¿Cómo? Supongamos que el no-terminal <code>T</code> todavía no ha aparecido porque esa reducción aún no ha ocurrido. Entonces lo que debería venir a continuación en la pila es algo que sea prefijo de alguna producción de <code>T</code>, de modo que un <strong>reduce</strong> futuro nos ponga ese <code>T</code> en la pila. En ese caso, todavía estaríamos en un prefijo viable, porque tendríamos un prefijo de <code>E -&gt; T</code>, y luego un prefijo de algo que se genera con <code>T</code>. ¿Cómo reconocer entonces cualquier prefijo de cualquier producción que sale de <code>T</code>? Pues afortunadamente tenemos estados que hacen justamente eso, dígase <code>T -&gt; .int</code> y <code>T -&gt; .int * T</code>, es decir, los <strong>items</strong> iniciales de las producciones de <code>T</code>. Dado que estamos construyendo un autómata no-determinista, tenemos la libertad de añadir transiciones <span class="math inline"><em>ϵ</em></span> a estos dos estados. De modo que el estado <code>E -&gt; .T</code> tiene tres transiciones, con un <code>T</code> se mueve a <code>E -&gt; T.</code>, y con <span class="math inline"><em>ϵ</em></span> se mueve a <code>T -&gt; .int</code> y a <code>T -&gt; .int * T</code>.</p>
<p>Por otro lado, si estuviéramos en el estado <code>E -&gt; T.+ E</code>, lo único que podemos esperar que venga en la pila es un terminal <code>+</code>. En cualquier otro ya no tendríamos un prefijo viable, pues estábamos esperando tener un prefijo de <code>E -&gt; T + E</code>, y ya hemos visto en la pila un <code>T</code>. Por tanto si fuera cierto que esta pila es un prefijo viable, tendría que venir algo que continuara este prefijo o empezara un nuevo prefijo. Pero dado que en la producción que estamos esperando lo que viene es un terminal, no existe forma de un <strong>reduce</strong> futuro nos ponga en esa posición a dicho terminal (los <strong>reduce</strong> siempre introducen un no-terminal en la pila). Luego, si no viene exactamente un <code>+</code> en la pila, ya podemos estar seguros que este prefijo no es viable (claro, como estamos en un autómata no-determinista puede que existan otros caminos donde sí se reconoce un prefijo viable).</p>
<p>De forma general tenemos las siguientes reglas:</p>
<ul>
<li>Si tenemos un estado <span class="math inline"><em>X</em> → <em>α</em>.<em>c</em><em>β</em></span> donde <span class="math inline"><em>c</em></span> es un terminal, añadimos una transición con <span class="math inline"><em>c</em></span> al estado <span class="math inline"><em>X</em> → <em>α</em><em>c</em>.<em>β</em></span>.</li>
<li>Si tenemos un estado <span class="math inline"><em>X</em> → <em>α</em>.<em>Y</em><em>β</em></span> donde <span class="math inline"><em>Y</em></span> es un no-terminal, añadimos una transición con <span class="math inline"><em>Y</em></span> al estado <span class="math inline"><em>X</em> → <em>α</em><em>Y</em>.<em>β</em></span>, y además por cada producción <span class="math inline"><em>Y</em> → <em>δ</em></span> añadimos una transición con <span class="math inline"><em>ϵ</em></span> al estado <span class="math inline"><em>Y</em> → .<em>δ</em></span>.</li>
</ul>
<p>Apliquemos entonces estas reglas al conjunto completo de <strong>items</strong> que hemos obtenido anteriormente. Primero definiremos un estado por cada <strong>item</strong>, y luego iremos adicionando las transiciones:</p>
<pre><code>[ 1]  E -&gt; .T + E     {  }
[ 2]  E -&gt;  T.+ E     {  }
[ 3]  E -&gt;  T +.E     {  }
[ 4]  E -&gt;  T + E.    {  }
[ 5]  E -&gt; .T         {  }
[ 6]  E -&gt;  T.        {  }
[ 7]  T -&gt; .int * T   {  }
[ 8]  T -&gt;  int.* T   {  }
[ 9]  T -&gt;  int *.T   {  }
[10]  T -&gt;  int * T.  {  }
[11]  T -&gt; .int       {  }
[12]  T -&gt;  int.      {  }
[13]  T -&gt; .( E )     {  }
[14]  T -&gt;  (.E )     {  }
[15]  T -&gt;  ( E.)     {  }
[16]  T -&gt;  ( E ).    {  }</code></pre>
<p>Tomemos entonces el estado <code>E -&gt; .T + E</code>. Primero ponemos la transición con <code>T</code> hacia <code>E -&gt; T.+ E</code>:</p>
<pre><code>[ 1]  E -&gt; .T + E     { T:2 }</code></pre>
<p>Y luego, dado que <code>T</code> es un no-terminal, adicionamos las transiciones <span class="math inline"><em>ϵ</em></span> correspondientes a los estados <code>T -&gt; .int</code> y <code>T -&gt; .int * T</code>:</p>
<pre><code>[ 1]  E -&gt; .T + E     { T:2, e:7, e:11 }</code></pre>
<p>Por otro lado, para <code>E -&gt; T.+ E</code> la única transición válida es con <code>+</code>, hacia el estado <code>E -&gt; T +.E</code>:</p>
<pre><code>[ 2]  E -&gt;  T.+ E     { +:3 }</code></pre>
<p>Para el estado <code>E -&gt; T+.E</code> igualmente tenemos una transición con <code>E</code> y dos transiciones con <span class="math inline"><em>ϵ</em></span>:</p>
<pre><code>[ 3]  E -&gt;  T +.E     { E:4, e:1, e:5 }</code></pre>
<p>El estado <code>E -&gt; T + E.</code> no tiene transiciones salientes, pues representa que se ha reconocido toda la producción. Es responsabilidad de otros estados continuar reconociendo (de forma no-determinista) los prefijos que puedan quedar en la pila.</p>
<p>El estado <code>E -&gt; .T</code> se parece mucho al estado <code>E -&gt; .T + E</code>. De hecho, tiene las mismas transiciones:</p>
<pre><code>[ 5]  E -&gt; .T         { T:2, e:7, e:11 }</code></pre>
<p>Finalmente el estado <code>E -&gt; T.</code> tampoco transiciones salientes. Ya hemos dicho que todos los estados son finales, pues como las transiciones siempre nos mueven de un prefijo viable a otro, en cualquier momento en que se acabe la pila tenemos un prefijo viable reconocido. Solo queda definir el estado inicial. En principio, deberíamos empezar de forma no-determinista por cualquiera de los estados iniciales de las producciones de <code>E</code>. Afortunadamente, en un autómata no-determinista tenemos un recurso para simular esta situación en la que queremos 2 estados iniciales. Simplemente añadimos un estado &quot;dummy&quot;, con transiciones <span class="math inline"><em>ϵ</em></span> a cada uno de los estados iniciales que deseamos. Desde el punto de la gramática, esto es equivalente a añadir un símbolo nuevo <code>E'</code> con la única producción <code>E' -&gt; E</code> y convertirlo en el símbolo inicial. Para esta producción tenemos dos nuevos <strong>items</strong>: <code>E' -&gt; .E</code> y <code>E' -&gt; E.</code>. El estado <code>E' -&gt; .E</code> se convertirá en el estado inicial de nuestro autómata.</p>
<p>El estado <code>E' -&gt; E.</code> también es convieniente, pues nos permite reconocer que hemos logrado reducir al símbolo inicial, y deberíamos haber terminado de consumir toda la cadena. De modo que este estado &quot;especial&quot; nos permitirá además saber cuando aceptar la cadena. No podemos simplemente aceptar la cadena en cualquier estado donde se reduzca a <code>E</code>, porque es posible que estuviéramos reduciendo a un <code>E</code> intermedio, por ejemplo, al <code>E</code> que luego de ser reducido en <code>T -&gt; (E)</code>.</p>
<p>Ahora que hemos visto como se construyen estas transiciones, veamos directamente el autómata completo.</p>
<pre><code>[ 0] E&#39; -&gt; .E         { E: 17, e:1, e:5 }
[ 1]  E -&gt; .T + E     { T:2, e:7, e:11 }
[ 2]  E -&gt;  T.+ E     { +:3 }
[ 3]  E -&gt;  T +.E     { E:4, e:1, e:5 }
[ 4]  E -&gt;  T + E.    {  }
[ 5]  E -&gt; .T         { T:2, e:7, e:11 }
[ 6]  E -&gt;  T.        {  }
[ 7]  T -&gt; .int * T   { int:8 }
[ 8]  T -&gt;  int.* T   { *:9 }
[ 9]  T -&gt;  int *.T   { T:10, e:7, e:11 }
[10]  T -&gt;  int * T.  {  }
[11]  T -&gt; .int       { int:12 }
[12]  T -&gt;  int.      {  }
[13]  T -&gt; .( E )     { (:14 }
[14]  T -&gt;  (.E )     { E:15, e:1, e:5 }
[15]  T -&gt;  ( E.)     { ):16 }
[16]  T -&gt;  ( E ).    {  }
[17] E&#39; -&gt;  E.</code></pre>
<p>A estos <strong>items</strong> se les denomina también <strong>items LR(0)</strong>, que significa <em>left-to-right rightmost-derivation look-ahead 0</em>.</p>
<h2 id="autómata-lr0">Autómata LR(0)</h2>
<p>Hemos construido finalmente un autómata finito no-determinista que reconoce exactamente el lenguaje de los prefijos viables. Sabemos que existe un autómata finito determinista que reconoce exactamente el mismo lenguaje. Aplicando el algoritmo de conversión de NFA a DFA podemos obtener dicho autómata. Sin embargo, hay una forma más directa de obtener el autómata finito determinista, que consiste en construir los estados aplicando el algoritmo de conversión a medida que vamos analizando las producciones y obteniendo los <strong>items</strong>.</p>
<p>Recordemos que el algoritmo de conversión de NFA a DFA básicamente construye un estado por cada subconjunto de los estados del NFA, siguiendo primero todas las transiciones con el mismo terminal, y luego computando la <span class="math inline"><em>ϵ</em></span>-clausura del conjunto de estados resultante. Para cada uno de estos &quot;super-estados&quot; <span class="math inline"><em>Q</em><sub><em>i</em></sub></span>, la nueva transición con un terminal concreto <span class="math inline"><em>c</em></span> va hacia el super-estado que representa exactamente al conjunto clausura de todos los estados originales a los que se llegaba desde algún estado <span class="math inline"><em>q</em><sub><em>j</em></sub> ∈ <em>Q</em><sub><em>i</em></sub></span>.</p>
<p>Vamos ahora a reescribir este algoritmo, pero teniendo en cuenta directamente que los estados del NFA son <strong>items</strong>. Por tanto, los super-estados del DFA serán conjuntos de <strong>items</strong>, que son justamente la <span class="math inline"><em>ϵ</em></span>-clausura de los <strong>items</strong> a los que se puede llegar desde otro conjunto de <strong>items</strong> siguiendo un símbolo concreto <span class="math inline"><em>X</em></span> (terminal o no-terminal). Definiremos entonces dos tipos de <strong>items</strong> para simplicar:</p>
<ul>
<li><p>Un <strong>item kernel</strong> es aquel de la forma <span class="math inline"><em>E</em>′→.<em>E</em></span> si <span class="math inline"><em>E</em>′</span> es nuevo símbolo inicial, o cualquier item de la forma <span class="math inline"><em>X</em> → <em>α</em>.<em>β</em></span> con <span class="math inline">|<em>α</em>|&gt;1</span>.</p></li>
<li><p>Un <strong>item no kernel</strong> es aquel de la forma <span class="math inline"><em>X</em> → .<em>β</em></span> excepto el <strong>item</strong> <span class="math inline"><em>E</em>′→.<em>E</em></span>.</p></li>
</ul>
<p>Hemos hecho estas definiciones, porque de cierta forma los <strong>items kernel</strong> son los realmente importantes. De hecho, podemos definir dado un conjunto de <strong>items kernel</strong>, el conjunto clausura, que simplemente añade todos los <strong>items no kernel</strong> que se derivan de este conjunto.</p>
<blockquote>
<p>Sea <span class="math inline"><em>I</em></span> un conjunto de <strong>items</strong> (kernel o no), el conjunto clausura de <span class="math inline"><em>I</em></span> se define como <span class="math inline"><em>C</em><em>L</em>(<em>I</em>)=<em>I</em> ∪ {<em>X</em> → .<em>β</em>}</span> tales que <span class="math inline"><em>Y</em> → <em>α</em>.<em>X</em><em>δ</em> ∈ <em>C</em><em>L</em>(<em>I</em>)</span>.</p>
</blockquote>
<p>Es decir, el conjunto clausura no es más que la formalización de la operación mediante la cuál añadimos todos los <strong>items</strong> no-kernel que puedan obtenerse de cualquier <strong>item</strong> en <span class="math inline"><em>I</em></span>. Nótese que la definición es recursiva, es decir, el conjunto clausura de <span class="math inline"><em>I</em></span> se define a partir del propio conjunto clausura de <span class="math inline"><em>I</em></span>. Para computarlo, simplemente partimos de <span class="math inline"><em>C</em><em>L</em>(<em>I</em>)=<em>I</em></span> y añadimos todos los items no-kernel que podamos, mientras cambie el conjunto. Por ejemplo, computemos el conjunto clausura del item asociado estado inicial <span class="math inline"><em>E</em>′→.<em>E</em></span>. Partimos del conjunto singleton que solo contiene a este item:</p>
<pre><code>I = { E&#39; -&gt; .E }</code></pre>
<p>Ahora buscamos todas las producciones de <code>E</code> y añadimos sus items iniciales:</p>
<pre><code>I = { E&#39; -&gt; .E,
       E -&gt; .T,
       E -&gt; .T + E }</code></pre>
<p>Ahora buscamos todas las producciones de <code>T</code> y añadimos sus items iniciales:</p>
<pre><code>I = { E&#39; -&gt; .E,
       E -&gt; .T,
       E -&gt; .T + E,
       T -&gt; .int,
       T -&gt; .int * T,
       T -&gt; .( E ) }</code></pre>
<p>Como no hemos añadido ningún item que tenga un punto delante de un no-terminal nuevo, este es el conjunto final. Notemos que esta definición no es nada más que la definición de <span class="math inline"><em>ϵ</em></span>-clausura usada en la conversión de un NFA a un DFA, solo que la hemos definido en función de los <strong>items</strong> directamente. Si aplicamos la <span class="math inline"><em>ϵ</em></span>-clausura al estado <span class="math inline"><em>q</em><sub>0</sub></span> de nuestro NFA definido anteriormente, llegaremos exactamente al mismo conjunto de <strong>items</strong>.</p>
<p>Una vez que tenemos este conjunto clausura de <strong>items</strong>, podemos definir entonces cómo añadir transiciones. Para ello definiremos la función <span class="math inline"><em>G</em><em>o</em><em>t</em><em>o</em>(<em>I</em>, <em>X</em>)=<em>J</em></span>, que nos mapea un conjunto de items a otro conjunto de items a partir de un símbolo <span class="math inline"><em>X</em></span>, de la siguiente forma:</p>
<blockquote>
<p><span class="math inline"><em>G</em><em>o</em><em>t</em><em>o</em>(<em>I</em>, <em>X</em>)=<em>C</em><em>L</em>({<em>Y</em> → <em>α</em><em>X</em>.<em>β</em>|<em>Y</em> → <em>α</em>.<em>X</em><em>β</em> ∈ <em>I</em>})</span></p>
</blockquote>
<p>La función <span class="math inline"><em>G</em><em>o</em><em>t</em><em>o</em>(<em>I</em>, <em>X</em>)</span> simplemente busca todos los items en <span class="math inline"><em>I</em></span> donde aparece un punto delante del símbolo <span class="math inline"><em>X</em></span>, crea un nuevo conjunto donde el punto aparece detrás del símbolo <span class="math inline"><em>X</em></span>, y luego calcula la clausura de este conjunto. Básicamente lo que estamos es formalizando la misma operación <span class="math inline"><em>G</em><em>o</em><em>t</em><em>o</em></span> que usábamos en la conversión de NFA a DFA, pero esta vez escrita en función de los <strong>items</strong>. Por ejemplo, si <span class="math inline"><em>I</em></span> es el conjunto calculado anteriormente, entonces:</p>
<pre><code>Goto(I,T) = { E -&gt; T., E -&gt; T. + E }</code></pre>
<p>Dado que no existe ningún punto delante de un no-terminal, no es necesario computar la clausura.</p>
<p>Una vez que tenemos estas dos definiciones, podemos dar un algoritmo para construir el autómata finito determinista que reconoce los prefijos viables. El estado inicial de nuestro autómata será justamente <span class="math inline"><em>C</em><em>L</em>(<em>E</em>′→.<em>E</em>)</span>. Luego, repetimos la siguiente operación mientras sea necesario: por cada estado <span class="math inline"><em>I</em></span> y cada símbolo <span class="math inline"><em>X</em></span>, añadimos el estado <span class="math inline"><em>G</em><em>o</em><em>t</em><em>o</em>(<em>I</em>, <em>X</em>)</span> si no existe, y añadimos la transición <span class="math inline"><em>I</em>→<sup><em>X</em></sup><em>J</em></span>. El algoritmo termina cuando no hay cambios en el autómata.</p>
<p>Apliquemos entonces este algoritmo a nuestra gramática para expresiones. Partimos del estado <span class="math inline"><em>I</em><sub>0</sub></span> ya computado:</p>
<pre><code>I0 = { E&#39; -&gt; .E,
        E -&gt; .T,
        E -&gt; .T + E,
        T -&gt; .int,
        T -&gt; .int * T,
        T -&gt; .( E ) }</code></pre>
<p>Calculemos ahora <span class="math inline"><em>G</em><em>o</em><em>t</em><em>o</em>(<em>I</em><sub>0</sub>, <em>E</em>)</span>:</p>
<pre><code>I1 = { E&#39; -&gt; E. }</code></pre>
<p>Como no hay ningún punto delante de un no-terminal, la clausura se mantiene igual. Calculemos entonces <span class="math inline"><em>G</em><em>o</em><em>t</em><em>o</em>(<em>I</em><sub>0</sub>, <em>T</em>)</span>:</p>
<pre><code>I2 = { E -&gt; T.,
       E -&gt; T.+ E }</code></pre>
<p>Igualmente la clausura no añade items. Calculemos ahora <span class="math inline"><em>G</em><em>o</em><em>t</em><em>o</em>(<em>I</em><sub>0</sub>, <em>i</em><em>n</em><em>t</em>)</span>:</p>
<pre><code>I3 = { T -&gt; int.,
       T -&gt; int.* T }</code></pre>
<p>Y ahora <span class="math inline"><em>G</em><em>o</em><em>t</em><em>o</em>(<em>I</em><sub>0</sub>, ()</span>:</p>
<pre><code>I4 = { T -&gt; (.E ) }</code></pre>
<p>A este estado si tenemos que calcularle su clausura:</p>
<pre><code>I4 = { T -&gt;  (.E ),
       E -&gt; .T,
       E -&gt; .T + E,
       T -&gt; .int,
       T -&gt; .int * T,
       T -&gt; .( E ) }</code></pre>
<p>De modo que ya terminamos con <span class="math inline"><em>I</em><sub>0</sub></span>. Dado que en <span class="math inline"><em>I</em><sub>1</sub></span> no hay símbolos tras un punto, calculemos entonces <span class="math inline"><em>G</em><em>o</em><em>t</em><em>o</em>(<em>I</em><sub>2</sub>, +)</span>, y aplicamos la clausura directamente:</p>
<pre><code>I5 = { E -&gt;  T +.E,
       E -&gt; .T,
       E -&gt; .T + E,
       T -&gt; .int,
       T -&gt; .int * T,
       T -&gt; .( E ) }</code></pre>
<p>Calculamos <span class="math inline"><em>G</em><em>o</em><em>t</em><em>o</em>(<em>I</em><sub>3</sub>, *)</span>:</p>
<pre><code>I6 = { T -&gt;  int *.T,
       T -&gt; .int,
       T -&gt; .int * T,
       T -&gt; .( E ) }</code></pre>
<p>Calculamos <span class="math inline"><em>G</em><em>o</em><em>t</em><em>o</em>(<em>I</em><sub>4</sub>, <em>E</em>)</span>:</p>
<pre><code>I7 = { T -&gt; ( E.) }</code></pre>
<p>Si ahora calculamos <span class="math inline"><em>G</em><em>o</em><em>t</em><em>o</em>(<em>I</em><sub>4</sub>, <em>T</em>)</span>, y nos daremos cuenta que es justamente <span class="math inline"><em>I</em><sub>2</sub></span>. Por otro lado, afortunadamente <span class="math inline"><em>G</em><em>o</em><em>t</em><em>o</em>(<em>I</em><sub>4</sub>, <em>i</em><em>n</em><em>t</em>)=<em>I</em><sub>3</sub></span>. Y finalmente <span class="math inline"><em>G</em><em>o</em><em>t</em><em>o</em>(<em>I</em><sub>4</sub>, ()</span> es el propio estado <span class="math inline"><em>I</em><sub>4</sub></span>! Por otro lado, <span class="math inline"><em>G</em><em>o</em><em>t</em><em>o</em>(<em>I</em><sub>5</sub>, <em>E</em>)</span> es:</p>
<pre><code>I8 = { E -&gt; T + E. }</code></pre>
<p>Mientras que <span class="math inline"><em>G</em><em>o</em><em>t</em><em>o</em>(<em>I</em><sub>5</sub>, <em>T</em>)</span> nos lleva de regreso a <span class="math inline"><em>I</em><sub>3</sub></span>, y <span class="math inline"><em>G</em><em>o</em><em>t</em><em>o</em>(<em>I</em><sub>5</sub>, ()</span> a <span class="math inline"><em>I</em><sub>4</sub></span>. Saltamos entonces para <span class="math inline"><em>G</em><em>o</em><em>t</em><em>o</em>(<em>I</em><sub>6</sub>, <em>T</em>)</span> que introduce un estado nuevo:</p>
<pre><code>I9 = { T -&gt; int * T. }</code></pre>
<p>Por otro lado, <span class="math inline"><em>G</em><em>o</em><em>t</em><em>o</em>(<em>I</em><sub>6</sub>, <em>i</em><em>n</em><em>t</em>)=<em>I</em><sub>3</sub></span> nuevamente, mientras que <span class="math inline"><em>G</em><em>o</em><em>t</em><em>o</em>(<em>I</em><sub>6</sub>, ()</span> nos regresa nuevamente a <span class="math inline"><em>I</em><sub>4</sub></span>. Finalmente, <span class="math inline"><em>G</em><em>o</em><em>t</em><em>o</em>(<em>I</em><sub>7</sub>, ))</span> nos da el siguiente, y último estado del autómata (ya que <span class="math inline"><em>I</em><sub>8</sub></span> e <span class="math inline"><em>I</em><sub>9</sub></span> no tienen transiciones salientes):</p>
<pre><code>I10 = { T -&gt; ( E ). }</code></pre>
<p>Para agilizar este algoritmo, podemos notar que, como dijimos anteriormente, solamente los item kernel son importantes. De hecho, podemos probar fácilmente que dos estados son iguales sí y solo si sus item kernel son iguales, dado que las operaciones de clausura sobre conjuntos de items kernel iguales nos darán el mismo conjunto final. Por lo tanto, en una implementación computacional (o un cómputo manual), si distinguimos los items kernel del resto de los items, cuando computamos un nuevo estado a partir de la función <span class="math inline"><em>G</em><em>o</em><em>t</em><em>o</em></span>, antes de computar su clausura vemos si su conjunto de items kernel coincide con el kernel de otro estado ya creado. En caso contrario, hemos descubierto un nuevo estado y pasamos a computar su clausura.</p>
<h2 id="parsing-lr0">Parsing LR(0)</h2>
<p>Una vez construido el autómata, podemos finalmente diseñar un algoritmo de parsing bottom-up. Este algoritmo se basa en la idea de verificar en cada momento si el estado de la pila es un prefijo viable, y luego, según el terminal que corresponda en <span class="math inline"><em>ω</em></span>, decidimos si la operación a realizar es <strong>shift</strong> o <strong>reduce</strong>. Para determinar si la pila es un prefijo viable, simplemente corremos el autómata construido en el contenido de la pila. Supongamos que este autómata se detiene en el estado <span class="math inline"><em>I</em></span>. Vamos que nos dicen los items de este estado sobre la operación más conveniente a realizar.</p>
<p>Si en este estado tenemos un item <span class="math inline"><em>X</em> ← <em>α</em>.<em>c</em><em>β</em></span>, y <span class="math inline"><em>c</em><em>ω</em></span> es la cadena de entrada (es decir, <span class="math inline"><em>c</em></span> es el próximo terminal a analizar), entonces es evidente que una operación de <strong>shift</strong> me seguirá manteniendo en la pila un prefijo viable. ¿Por qué? Pues porque al hacer <strong>shift</strong> el contenido de la pila ahora crece en <span class="math inline"><em>c</em></span>, y si vuelvo a correr el autómata desde el inicio de la pila, llegaré nuevamente al estado <span class="math inline"><em>I</em></span> justo antes de analizar <span class="math inline"><em>c</em></span>. Pero sé que desde <span class="math inline"><em>I</em></span> hay una transición con <span class="math inline"><em>c</em></span> a cierto estado <span class="math inline"><em>J</em></span>, que es justamente <span class="math inline"><em>G</em><em>o</em><em>t</em><em>o</em>(<em>I</em>, <em>c</em>)</span>, por lo tanto terminaré en el estado <span class="math inline"><em>J</em></span> habiendo leído toda la pila. Luego, por definición de prefijo viable, como he podido reconocer el contenido de la pila, todo está bien.</p>
<p>Por otro lado, si en el estado <span class="math inline"><em>I</em></span> tengo un item de la forma <span class="math inline"><em>X</em> ← <em>β</em>.</span>, entonces es conveniente hacer una operación de <strong>reduce</strong> justamente en la producción <span class="math inline"><em>X</em> → <em>β</em></span>. Para ver por qué esta operación me sigue manteniendo en la pila un prefijo viable, notemos que <span class="math inline"><em>X</em> → <em>β</em>.</span> quiere decir que hemos reconocido en la pila toda la parte derecha de esta producción. Entonces en la pila lo que tenemos en un <strong>handle</strong>, y por su propia definición reducir en un <strong>handle</strong> siempre es correcto.</p>
<p>De modo que tenemos un algoritmo. En cada iteración, corremos el autómata en el contenido de la pila, y analizamos cuál de las estrategias anteriores es válida según el contenido del estado en que termina el autómata. Si en algún momento el autómata no tiene una transición válida, tiene que ser con el último terminal que acabamos de hacer <strong>shift</strong> (ya que de lo contrario se hubiera detectado en una iteración anterior). Luego, este algoritmo reconoce los errores sintácticos lo antes posible. Es decir, nunca realiza una reducción innecesaria.</p>
<p>Por otro lado, puede suceder que en un estado del autómata tenga items que nos sugieran operaciones contradictorias. Llamaremos a estas situaciones, <strong>conflictos</strong>. En general, podemos tener 2 tipos de conflictos:</p>
<ul>
<li>Conflicto <strong>shift-reduce</strong> si ocurre que tengo un item que me sugiere hacer <strong>shift</strong> y otro que me sugiere hacer <strong>reduce</strong>.</li>
<li>Conflicto <strong>reduce-reduce</strong> si ocurre que tengo dos items que me sugieren hacer <strong>reduce</strong> a producciones distintas.</li>
</ul>
<p>En cualquiera de estos casos, tenemos una fuente de no-determinismo, pues no sabemos por cuál de estas operaciones se pudiera reconocer la cadena. Este no-determinismo se debe a que en el autómata no-determinista había más de un camino posible que reconocía la cadena, y al convertirlo a determinista, estos caminos se expresan como items contradictorios en el mismo estado. En estos casos, decimos que la gramática no es LR(0). Luego:</p>
<blockquote>
<p>Sea <span class="math inline"><em>G</em> = &lt;<em>S</em>, <em>N</em>, <em>T</em>, <em>P</em>&gt;</span> una gramática libre del contexto, <span class="math inline"><em>G</em></span> es LR(0) si y solo si en el autómata LR(0) asociado no existen conflictos <strong>shift-reduce</strong> ni conflictos <strong>reduce-reduce</strong>.</p>
</blockquote>
<p>Notemos que no es posible que tengamos conflictos <strong>shift-shift</strong>, pues solamente hay un caracter <span class="math inline"><em>c</em></span> en la cadena <span class="math inline"><em>ω</em></span>, y por tanto hay un solo estado hacia donde hacer <strong>shift</strong>.</p>
<p>Desgraciadamente nuestra gramática favorita de expresiones no es LR(0). Sin ir más lejos, en el estado <span class="math inline"><em>I</em><sub>3</sub></span> tenemos un conflicto <strong>shift-reduce</strong>. Podemos reducir <code>T -&gt; int</code>, o hacer <strong>shift</strong> si viene un terminal <code>*</code>. Intuitivamente el problema es que la operación de <strong>reduce</strong> es demasiado permisiva. Donde quiera que encontremos un item <strong>reduce</strong> diremos que es conveniente reducir en esa producción, aunque sabemos que esto no siempre es cierto. De hecho, ya hemos tenido que lidiar con este problema anteriormente, en el algoritmo de parsing LL.</p>
<h2 id="parsing-slr1">Parsing SLR(1)</h2>
<p>Recordemos que en el parsing LL teníamos la duda de cuando era conveniente aplicar una producción <span class="math inline"><em>X</em> → <em>ϵ</em></span>, y definimos para ello el conjunto <span class="math inline"><em>F</em><em>o</em><em>l</em><em>l</em><em>o</em><em>w</em>(<em>X</em>)</span>, que justamente nos decía donde era conveniente eliminar <span class="math inline"><em>X</em></span>. Pues en este caso, este conjunto también nos ayudará. Intuitivamente, si tenemos <span class="math inline"><em>X</em> → <em>β</em>.</span>, solamente tiene sentido reducir si en el <span class="math inline"><em>F</em><em>o</em><em>l</em><em>l</em><em>o</em><em>w</em>(<em>X</em>)</span> aparece el terminal que estamos analizando. ¿Por qué? Pues porque de lo contrario no es posible que lo que nos quede en la pila sea un prefijo viable.</p>
<p>Supongamos que <span class="math inline"><em>c</em></span> es el terminal a analizar, <span class="math inline"><em>c</em> ∉ <em>F</em><em>o</em><em>l</em><em>l</em><em>o</em><em>w</em>(<em>X</em>)</span> y hacemos la reducción. Entonces en el próximo <strong>shift</strong> tendremos en el tope de la pila la forma oracional <span class="math inline"><em>X</em><em>c</em></span>. Pero esta forma oracional no puede aparecer en niguna derivación extrema derecha, porque de lo contrario <span class="math inline"><em>c</em></span> sería parte del <span class="math inline"><em>F</em><em>o</em><em>l</em><em>l</em><em>o</em><em>w</em>(<em>X</em>)</span>. Por tanto, si esta forma oracional no es válida, entonces ningún <strong>handle</strong> puede tener este prefijo. Por tanto ya no tenemos un prefijo viable. Incluso si lo siguiente que hacemos tras reducir en <span class="math inline"><em>X</em></span> no es <strong>shift</strong> sino otra secuencia de operaciones <strong>reduce</strong>, en cualquier caso si lo que queda una vez hagamos <strong>shift</strong> es un prefijo viable, entonces es porque <span class="math inline"><em>c</em> ∈ <em>F</em><em>o</em><em>l</em><em>l</em><em>o</em><em>w</em>(<em>X</em>)</span> (intuitivamente, aplicando las producciones en las que redujimos hasta que vuelva a aparecer X, obtendremos la forma oracional <span class="math inline"><em>X</em><em>c</em></span> nuevamente).</p>
<p>Justamente a esta estrategia denominaremos SLR(1), o <em>Simple LR look-ahead 1</em>, dado que usamos un terminal de look-ahead para decidir si vale la pena reducir. Con esta estrategia, podemos comprobar que ya en el estado <span class="math inline"><em>I</em><sub>2</sub></span> no hay conflicto, pues <span class="math inline">* ∉ <em>F</em><em>o</em><em>l</em><em>l</em><em>o</em><em>w</em>(<em>T</em>)</span>, porque cuando viene un terminal <span class="math inline">*</span> solo tiene sentido hacer <strong>shift</strong>, nunca <strong>reduce</strong>.</p>
<p>De forma análoga llamamos gramáticas SLR(1) a aquellas gramáticas donde, bajo estas reglas, no existen conflictos.</p>
<p>Intentemos entonces reconocer la cadena <code>int * ( int + int )</code> con nuestro parser SLR(1). Comenzamos por el estado inicial:</p>
<pre><code>|int * ( int + int )</code></pre>
<p>Como la pila está vacía, el autómata termina en el estado <span class="math inline"><em>I</em><sub>0</sub></span>. Dado que viene un terminal <code>int</code>, buscamos la transición correspondiente, que es justamente hacia el estado <span class="math inline"><em>G</em><em>o</em><em>t</em><em>o</em>(<em>I</em><sub>0</sub>, <em>i</em><em>n</em><em>t</em>)=<em>I</em><sub>3</sub></span>. Por tanto, como existe esta transición, significa que la acción a realizar es <strong>shift</strong>.</p>
<pre><code> int|* ( int + int )</code></pre>
<p>Ahora corremos nuevamente el autómata, ya sabemos que caerá en el estado <span class="math inline"><em>I</em><sub>3</sub></span>. Ahora podemos potencialmente reducir o hacer <strong>shift</strong>. Calculamos el <span class="math inline"><em>F</em><em>o</em><em>l</em><em>l</em><em>o</em><em>w</em>(<em>T</em>)</span></p>
<pre><code>Follow(T) = { +, ), $ }</code></pre>
<p>Por tanto, como <code>*</code> no está incluido en el <code>Follow(T)</code>, no hay conflicto, solamente no queda hacer <strong>shift</strong>, en este caso al estado <span class="math inline"><em>I</em><sub>6</sub></span>.</p>
<pre><code> int *|( int + int )</code></pre>
<p>Corremos de nuevo y sabemos que acabaremos en <span class="math inline"><em>I</em><sub>6</sub></span>. Aquí no hay reducciones, así que solo queda hacer <strong>shift</strong> hacia el estado <span class="math inline"><em>I</em><sub>4</sub></span>:</p>
<pre><code> int * (|int + int )</code></pre>
<p>En <span class="math inline"><em>I</em><sub>4</sub></span> tampoco hay reducciones, así que hacemos <strong>shift</strong> hacia el estado <span class="math inline"><em>I</em><sub>3</sub></span>:</p>
<pre><code> int * ( int|+ int )</code></pre>
<p>Ahora interesantemente si tenemos que <code>+</code> pertenece al <code>Follow(X)</code>, por tanto la reducción aplica. Afortunadamente no hay transiciones en este estado con <code>+</code>, por lo que no hay conflicto. Aplicamos entonces la reducción:</p>
<pre><code> int * ( T|+ int )</code></pre>
<p>Ahora corremos el autómata nuevamente desde el inicio, siguiendo las transiciones (recordemos que mienstras estamos leyendo el contenido de la pila no nos importan los items). Terminamos en el estado <span class="math inline"><em>I</em><sub>2</sub></span>. En este estado podemos reducir a <code>E</code> o hacer <strong>shift</strong>. Pero resulta que <code>Follow(E)</code> no contiene al terminal <code>+</code>, por lo que la reducción no tiene sentido. Hacemos <strong>shift</strong> entonces:</p>
<pre><code> int * ( T +|int )</code></pre>
<p>Ahora el autómata termina en el estado <span class="math inline"><em>I</em><sub>5</sub></span>. En este estado, viniendo <code>int</code>, solamente tiene sentido hacer <strong>shift</strong> hacia el estado <span class="math inline"><em>I</em><sub>3</sub></span>:</p>
<pre><code> int * ( T + int|)</code></pre>
<p>Ahora estamos en una situación conocida. Pero en este caso, <code>)</code> sí está en el <code>Follow(T)</code>, y no hay transiciones con este símbolo, luego lo que queda es reducir:</p>
<pre><code> int * ( T + T|)</code></pre>
<p>Al correr el autómata, en vez de <span class="math inline"><em>I</em><sub>3</sub></span> como en la última vez, ahora de <span class="math inline"><em>I</em><sub>5</sub></span> pasaríamos directamente a <span class="math inline"><em>I</em><sub>2</sub></span>, donde nuevamente estamos en territorio conocido. Sin embargo, de nuevo en este caso <code>)</code> sí está en el <code>Follow(E)</code>, luego podemos reducir (y no hay transiciones con <code>)</code>):</p>
<pre><code> int * ( T + E|)</code></pre>
<p>Ahora volvemos a correr el autómata, pero en vez de el estado <span class="math inline"><em>I</em><sub>2</sub></span>, terminaríamos en el estado <span class="math inline"><em>I</em><sub>8</sub></span>, donde la única opción es reducir (una vez comprobamos el <code>Follow(E)</code>):</p>
<pre><code> int * ( E|)</code></pre>
<p>En este caso, al correr el autómata desde el inicio, terminamos en <span class="math inline"><em>I</em><sub>7</sub></span>, que nos dice <strong>shift</strong>:</p>
<pre><code> int * ( E )|</code></pre>
<p>Ahora de <span class="math inline"><em>I</em><sub>7</sub></span> saltaríamos para <span class="math inline"><em>I</em><sub>10</sub></span>, que nos indica la reducción (dado que <code>$</code> sí está en el <code>Follow(X)</code>):</p>
<pre><code> int * T|</code></pre>
<p>En este caso, rápidamente caeremos en el estado <span class="math inline"><em>I</em><sub>9</sub></span>, que nos indica reducir:</p>
<pre><code> T|</code></pre>
<p>El autómata con esta pila termina en el estado <span class="math inline"><em>I</em><sub>2</sub></span> nuevamente, pero ahora <code>$</code> es el terminal a analizar, por lo que hacemos la reducción:</p>
<pre><code> E|</code></pre>
<p>Y finalmente, en esta entrada el autómata nos deja en el estado <span class="math inline"><em>I</em><sub>1</sub></span>, que nos permite reducir por completo al símbolo especial <code>E'</code> y aceptar la cadena:</p>
<pre><code> E&#39;|</code></pre>
<p>De esta forma, el algoritmo de parsing SLR(1) ha logrado obtener una derivación extrema derecha de nuestra cadena favorita, pero empleando una gramática mucho más expresiva y &quot;natural&quot; que la gramática LL correspondiente.</p>
<p>De todas formas, muchas gramáticas medianamente complicadas no son SLR(1), por lo que necesitaremos un parser de mayor potencia. Para ello, tendremos que refinar aún más el criterio con el cuál se producen los <strong>reduce</strong>.</p>
<h2 id="parsing-lr1">Parsing LR(1)</h2>
<p>Veamos a continuación un ejemplo de una gramática clásica que no es SLR(1):</p>
<pre><code>S -&gt; E
E -&gt; A = A | i
A -&gt; i + A | i</code></pre>
<p>Esta gramática representa un subconjunto del lenguaje de las ecuaciones algebraicas, donde tanto en la parte derecha como en la izquierda del token <code>=</code> podemos tener una expresión aritmética cualquiera. Veamos que sucede al construir el autómata SLR(1):</p>
<pre><code>I0 = {
    S -&gt; .E
    E -&gt; .A = A
    E -&gt; .i
    A -&gt; .i + A
    A -&gt; .i
}</code></pre>
<p>Viendo los items de este estado, ya podemos intuir dónde podría haber problemas. Al hacer <code>Goto(I0, i)</code> aparecerán dos items <strong>reduce</strong> con parte izquierda distinta:</p>
<pre><code>Goto(I0, i) = {
    E -&gt; i.
    A -&gt; i.+ A
    A -&gt; i.
}</code></pre>
<p>En este estado aperece entonces un conflicto <strong>reduce-reduce</strong>, ya que <span class="math inline"><em>F</em><em>o</em><em>l</em><em>l</em><em>o</em><em>w</em>(<em>E</em>)={$}</span>, y <span class="math inline">$ ∈ <em>F</em><em>o</em><em>l</em><em>l</em><em>o</em><em>w</em>(<em>A</em>)</span>, puesto que <code>A</code> aparece como parte derecha de una producción de <code>E</code>. Por tanto esta gramática no es SLR(1). Sin embargo, la gramática no es ambigua, y esto es fácil de demostrar. Intuitivamente, la única cadena donde pudiera haber ambiguedad es justamente la cadena <code>i</code> (es el único token que es generado por más de un no-terminal). Sin embargo, para esta cadena, la única derivación posible es <code>S -&gt; E -&gt; i</code>. Aunque <code>A -&gt; i</code> es una producción, la forma oracional <code>i</code> no es <strong>handle</strong> de <code>A</code>. Si solo existe un <code>i</code> en la pila, este tiene que ser generado por el no-terminal <code>E</code>, pues de lo contrario no sería posible reducir a <code>S</code>.</p>
<p>Sin embargo, nuestro parser SLR(1) no es suficientemente inteligente para determinar esto. Al encontrarse con la forma oracional <code>i</code> en la pila, en principio, el autómata dice que <code>A -&gt; i</code> es una reducción posible. Sin embargo, sabemos que esta reducción es inválida, porque luego quedaría <code>A</code> en la pila, que no es una forma oracional válida en ninguna derivación extrema derecha. De la producción <code>E -&gt; A = A</code> podemos ver que esta gramática nunca genera una <code>A</code> sola. En otras palabras, nuestra heurística SLR(1) para detectar <strong>handles</strong> (reducir en <code>X</code> para todo terminal en el <code>Follow(X)</code>) es demasiado débil para manejar esta situación, y produce un falso positivo, al determinar que la forma oracional <code>ì$</code> es un <strong>handle</strong> de <code>A</code>, cuando realmente no lo es.</p>
<p>La pregunta es entonces, ¿por qué surge este conflicto? Qué falla en la heurística SLR(1) que produce estos falsos positivos? Evidentemente el conjunto Follow es en ocasiones demasiado grande, y contiene tokens para los cuáles no es válida una operación <strong>reduce</strong> en ese estado particular. Tratemos de rastrear, durante la construcción del autómata, dónde es que se introducen estos tokens inválidos.</p>
<p>Comenzamos por el estado inicial nuevamente, pero viéndolo paso a paso a medida que se computa la clausura. Comenzamos por el <em>kernel</em>:</p>
<pre><code>I0 = {
    S -&gt; .E
    ...
}</code></pre>
<p>En este punto, el único item de este estado indica que esperamos encontrar en la cadena una forma oracional que se reduzca a <code>E</code>. Por tanto, añadimos las producciones de <code>E</code>:</p>
<pre><code>I0 = {
    S -&gt; .E
    E -&gt; .A = A
    E -&gt; .i
    ...
}</code></pre>
<p>Hasta aquí no hay problemas, pues ni siquiera hay dos items con la misma parte derecha. Entonces tenemos que adicionar las producciones de <code>A</code>:</p>
<pre><code>I0 = {
    S -&gt; .E
    E -&gt; .A = A
    E -&gt; .i
    A -&gt; .i + A
    A -&gt; .i
}</code></pre>
<p>Y aquí es donde podemos tener la primera pista de que viene un conflicto. Tenemos dos items que tienen la misma parte derecha (<code>E -&gt; .i</code> y <code>A -&gt; .i</code>). Por tanto, tras el próximo <strong>shift</strong> llegaremos a un estado con dos <strong>reduce</strong> a no-terminales distintos. Ahora, recordemos que el conflicto va a suceder porque <code>$</code> está en la intersección de los Follow. Sin embargo, veamos por qué motivo aparece <code>A -&gt; .i</code> en este estado. Justamente, es por culpa de la producción <code>E -&gt; .A = A</code> que tenemos que expandir los items de <code>A</code>. Recordemos entonces qué significa este item para nosotros: básicamente representa que a partir de este punto en la cadena de entrada, estamos esperando que aparezca <em>algo que se pueda reducir</em> a la forma oracional <code>A = A</code>. Por tanto, como esta forma oracional empieza con <code>A</code>, debemos expandir sus producciones.</p>
<p>Pero sí analizamos cuidadosamente el significado del item anterior, veremos que no tiene sentido reducir a este <code>A</code> si aparece <code>$</code> como <em>look-ahead</em>. ¿Por qué? Pues precisamente, el item nos dice que lo que viene en la cadena, si resultara que es correcta, debería reducirse a la forma oracional <code>A = A</code>. Por tanto, la primera parte de esa supuesta cadena, que se debería reducir al primer <code>A</code> de la forma oracional, solamente sería correcta si justo detrás de ese <code>A</code> viniera un token <code>=</code>. De lo contrario no podríamos seguir reduciendo el resto de la forma oracional.</p>
<p>Es decir, en el momento qué por culpa del item <code>E -&gt; .A = A</code> nos toca adicionar el item <code>A -&gt; .i</code>, lo que estamos esperando es que ese <code>i</code> se reduzca a <code>A</code> e inmediatamente después venga un <code>=</code>. El propio item nos está diciendo eso. Por tanto, si en el siguiente estado resultara que felizmente apareció el token <code>i</code> en la pila, antes de reducirlo ingenuamente a <code>A</code>, deberíamos verificar si justo detrás viene el <code>=</code> que estábamos esperando. De lo contrario, podríamos decir que la reducción no tiene sentido, porque el item &quot;padre&quot; de este item (es decir, <code>E -&gt; .A = A</code>) se va a quedar esperando un <code>=</code> que no viene en la cadena.</p>
<p>El problema con la heurística SLR radica justamente en que calculamos el Follow de cualquier no-terminal de forma <em>global</em>. Es decir, no tenemos en cuenta qué según las producciones que se vayan aplicando, solamente una parte de ese Follow es la que realmente puede aparecer a continuación. En este caso particular <span class="math inline"><em>F</em><em>o</em><em>l</em><em>l</em><em>o</em><em>w</em>(<em>A</em>)={=, $}</span>. Pero cada uno de esos tokens está en el Follow por un motivo <em>distinto</em>. Justamente <code>=</code> aparece en el <code>Follow(A)</code> por culpa de la <em>primera</em> <code>A</code> en la producción <code>E -&gt; A = A</code>. Pero <code>$</code> aparece por culpa de la <em>segunda</em> <code>A</code> de esa producción. De cierta forma, es como si tuviéramos <em>dos instancias</em> distintas del mismo no-terminal <code>A</code>, aquella que aparece delante del token <code>=</code> y aquella que aparece detrás. Entonces cuando estamos parseando una cadena, y el autómata pasa al estado <code>E -&gt; .A = A</code>, estamos esperando una <code>A</code>, pero no cualquier <code>A</code>, sino aquella <em>instancia</em> de <code>A</code> que viene delante del <code>=</code>. El error en la heurística SLR es justamente que no puede identificar estas situaciones distintas. Para el autómata SLR toda <code>A</code> es la misma <code>A</code>, por tanto tiene sentido reducir siempre con el mismo <em>look-ahead</em>.</p>
<p>¿Cómo podemos entonces identificar de forma distinta a cuál de las posibles <code>A</code> nos estamos refiriendo? Precisamente, durante la construcción del autómata, cuando el item <code>E -&gt; .A = A</code> nos genera un item <code>A -&gt; .i</code>, sabemos a &quot;cuál&quot; <code>A</code> nos referimos. Es justamente aquella que estaba detrás del punto. Por tanto, podemos en este momento decir qué es lo que puede venir detrás de esa <code>A</code> particular si se aplica esta producción particular. Lo que haremos entonces es adicionar a cada item un conjunto de tokens, que nos dirán explícitamente cuándo es que tiene sentido hacer <strong>reduce</strong>. Y este conjunto de tokens lo iremos calculando a medida que se crean los nuevos items, justamente mirando en el item &quot;padre&quot; que lo generó qué es lo que puede venir detrás de cada no-terminal.</p>
<p>Vamos a introducir entonces el concepto de <strong>item LR(1)</strong>, que no es más que un <strong>item LR(0)</strong> normal junto a token:</p>
<blockquote>
<p>Sea <span class="math inline"><em>G</em> = &lt;<em>S</em>, <em>N</em>, <em>T</em>, <em>P</em>&gt;</span> una gramática libre del contexto, un <strong>item LR(1)</strong> es una expresión de la forma <span class="math inline"><em>X</em> → <em>α</em> ⋅ <em>β</em>, <em>c</em></span> donde <span class="math inline"><em>X</em> → <em>α</em><em>β</em></span> es una producción (<span class="math inline"><em>α</em>, <em>β</em> ∈ (<em>N</em> ∪ <em>T</em>)<sup>*</sup></span>), y <span class="math inline"><em>c</em> ∈ <em>T</em></span>.</p>
</blockquote>
<p>El nuevo token que hemos adicionado a cada item nos servirá para ir rastreando qué terminales pueden aparecer el Follow de la forma oracional que estamos intentando reducir. De modo que un item de la forma <span class="math inline"><em>X</em> → <em>α</em> ⋅ <em>β</em>, <em>c</em></span> en un estado del autómata representa que ya hemos reconocido la parte <span class="math inline"><em>α</em></span> de la forma oracional, y esperamos reconocer la parte <span class="math inline"><em>β</em></span>, y que una vez reconocida toda esta porción <span class="math inline"><em>β</em></span>, esperamos que venga exactamente un terminal <span class="math inline"><em>c</em></span>. Por tanto, en algún momento tendremos un item <span class="math inline"><em>X</em> → <em>δ</em> ⋅ ,<em>c</em></span>, y entonces la operación de <strong>reduce</strong> la aplicaremos solamente si el siguiente terminal es <span class="math inline"><em>c</em></span>. A este token asociado a cada item le llamaremos <em>look-ahead</em>, y a la parte <span class="math inline"><em>X</em> → <em>α</em> ⋅ <em>β</em></span> le llamaremos centro. De modo que un item LR(1) está compuesto por un centro, que es un item LR(0), y un token <em>look-ahead</em>.</p>
<p>A partir de este nuevo tipo de item, vamos a construir un autómata similar al SLR(1), que llamaremos <strong>LR(1) canónico</strong> o simplemente LR(1). Para ello, necesitaremos definir cuál es el item inicial, y cómo se computan nuevos items a partir de los items ya existentes. El item LR(1) inicial es fácil de definir. El centro es idéntico al item inicial del autómata SLR: es decir, <span class="math inline"><em>S</em> → ⋅<em>E</em></span> (siendo <code>S</code> el nuevo símbolo inicial de la gramática aumentada, y <code>E</code> el símbolo inicial de la gramática original). Ahora, para definir el token <em>look-ahead</em>, volvamos al significado de este item. Básicamente <span class="math inline"><em>S</em> → ⋅<em>E</em></span> significa que esperamos encontrar una forma oracional que se pueda reducir a <code>E</code>, y por tanto lo único que puede venir posteriormente es justamente <code>$</code>. De este modo el item LR(1) inicial significamente exactamente que queremos reducir <strong>toda</strong> la cadena a un simbolo <code>E</code>.</p>
<blockquote>
<p>Sea <span class="math inline"><em>G</em> = &lt;<em>S</em>, <em>N</em>, <em>T</em>, <em>P</em>&gt;</span> una gramática libre del contexto, <span class="math inline"><em>S</em>′</span> el símbolo inicial de la gramática autmentada, entonces el item LR(1) inicial es <span class="math inline"><em>S</em>′→ ⋅ <em>S</em>, $</span>.</p>
</blockquote>
<p>Veamos entonces qué sucede con cualquier otro item LR(1). Recordemos que en SLR teníamos dos tipos de items, los <em>kernel</em> y los <em>no-kernel</em>. Aquí tendremos la misma separación. Si tenemos un item LR(1) <span class="math inline"><em>X</em> → <em>α</em> ⋅ <em>x</em><em>β</em>, <em>c</em></span>, donde <span class="math inline"><em>x</em> ∈ <em>T</em></span>, la operación <strong>shift</strong> nos generará el item <span class="math inline"><em>X</em> → <em>e</em><em>s</em><em>α</em><em>x</em> ⋅ <em>β</em>, <em>c</em></span>. Dado que todavía estamos reconociendo la parte derecha de esta producción, el <em>look-ahead</em> se mantiene igual. Por otro lado, si tenemos <span class="math inline"><em>X</em> → <em>α</em> ⋅ <em>Y</em><em>β</em>, <em>c</em></span> con <span class="math inline"><em>Y</em> ∈ <em>N</em></span>, entonces tenemos que computar (además de correr el punto) todos los items con centro <span class="math inline"><em>Y</em> → ⋅<em>δ</em></span>. La pregunta es entonces cuál es el <em>look-ahead</em> correspondiente. Básicamente la pregunta es, si logramos reducir a <span class="math inline"><em>δ</em></span> al <span class="math inline"><em>Y</em></span> que estamos esperando, ¿qué puede venir detrás? La respuesta es, en principio, todo terminal en el <span class="math inline"><em>F</em><em>i</em><em>r</em><em>s</em><em>t</em>(<em>β</em>)</span>, y <em>nada más</em>.</p>
<p>Ahora, puede suceder que <span class="math inline"><em>β</em>→<sup>*</sup><em>ϵ</em></span>, es decir, que la parte detrás de <span class="math inline"><em>Y</em></span> no exista, o que desaparezca en el futuro. En este caso, tendremos que <span class="math inline"><em>ϵ</em> ∈ <em>F</em><em>i</em><em>r</em><em>s</em><em>t</em>(<em>β</em>)</span>. Pero no tiene sentido alguno decir que detrás de <span class="math inline"><em>Y</em></span> esperamos que venga <span class="math inline"><em>ϵ</em></span> (incluso definimos anteriormente que <span class="math inline"><em>ϵ</em> ∉ <em>F</em><em>o</em><em>l</em><em>l</em><em>o</em><em>w</em>(<em>Y</em>)</span> para todo <span class="math inline"><em>Y</em></span>). Sin embargo, en este caso, si <span class="math inline"><em>β</em></span> desaparece, entonces lo único que puede venir detrás de <span class="math inline"><em>Y</em></span>, es justamente el <em>look-ahead</em> de <span class="math inline"><em>X</em></span>. Por ejemplo, para el item <span class="math inline"><em>X</em> → <em>α</em> ⋅ <em>Y</em>, <em>c</em></span> generamos el item <span class="math inline"><em>Y</em> → ⋅<em>δ</em>, <em>c</em></span>, pues como <span class="math inline"><em>Y</em></span> está al final del <span class="math inline"><em>X</em></span>, le sigue lo mismo que habíamos decidido que seguía a <span class="math inline"><em>X</em></span>. Podemos generalizar de la siguiente manera, extendiendo los conceptos definidos para SLR:</p>
<blockquote>
<p>Sea <span class="math inline"><em>I</em></span> un conjunto de <strong>items LR(1)</strong> (kernel o no), el conjunto clausura de <span class="math inline"><em>I</em></span> se define como <span class="math inline"><em>C</em><em>L</em>(<em>I</em>)=<em>I</em> ∪ {<em>X</em> → .<em>β</em>, <em>b</em>}</span> tales que <span class="math inline"><em>Y</em> → <em>α</em>.<em>X</em><em>δ</em>, <em>c</em> ∈ <em>C</em><em>L</em>(<em>I</em>)</span> y <span class="math inline"><em>b</em> ∈ <em>F</em><em>i</em><em>r</em><em>s</em><em>t</em>(<em>δ</em><em>c</em>)</span>.</p>
</blockquote>
<blockquote>
<p>Sea <span class="math inline"><em>I</em></span> un conjunto de <strong>items LR(1)</strong>, se define la función <span class="math inline"><em>G</em><em>o</em><em>t</em><em>o</em>(<em>I</em>, <em>X</em>)=<em>C</em><em>L</em>({<em>Y</em> → <em>α</em><em>X</em>.<em>β</em>, <em>c</em>|<em>Y</em> → <em>α</em>.<em>X</em><em>β</em>, <em>c</em> ∈ <em>I</em>})</span></p>
</blockquote>
<p>Para construir el autómata, seguimos el mismo algoritmo que para SLR. La función <strong>Goto</strong> para un conjunto de items (un estado) se define de igual forma como el conjunto de items (estado) que se obtienen de aplicar <strong>Goto</strong> a cada item en el conjunto origen. La función <strong>Clausura</strong> igualmente se define de forma recursiva como la clausura transitiva de cada uno de los items del propio conjunto. De la misma forma, tendremos conflictos <strong>shift-reduce</strong> o <strong>reduce-reduce</strong> en algún estado, si los <em>look-ahead</em> de algunas producciones <strong>reduce</strong> coinciden con otras, o con una de las transiciones salientes.</p>
<p>Pasemos entonces a construir el autómata LR(1) de la gramática anterior:</p>
<pre><code>S -&gt; E
E -&gt; A = A | i
A -&gt; i + A | i</code></pre>
<p>Comenzamos por el estado inicial:</p>
<pre><code>I0 = {
    S -&gt; .E, $
}</code></pre>
<p>Vamos a añadir entonces las producciones de <code>E</code>. Notemos que como en <code>S -&gt; E</code> detrás de <code>E</code> no viene nada, el <em>look-ahead</em> será justamente <code>$</code>:</p>
<pre><code>I0 = {
    S -&gt; .E,     $
    E -&gt; .A = A, $
    E -&gt; .i,     $
}</code></pre>
<p>Hasta el momento hemos obtenido un estado inicial equivalente al del autómata SLR correspondiente. Adicionamos entonces las producciones de <code>A</code>, y veremos el primer cambio importante. El centro de estos items serán (de forma equivalente al caso SLR), <code>A -&gt; .i + A</code> y <code>A -&gt; .i</code>. Sin embargo, al calcular el <em>look-ahead</em>, aplicando la definición, tenemos que computar <span class="math inline"><em>F</em><em>i</em><em>r</em><em>s</em><em>t</em>(=$)</span> que es el token <code>=</code>. Luego:</p>
<pre><code>I0 = {
    S -&gt; .E,     $
    E -&gt; .A = A, $
    E -&gt; .i,     $
    A -&gt; .i + A, =
    A -&gt; .i,     =
}</code></pre>
<p>Y ya podemos intuir cómo el autómata LR resolverá el conflicto <strong>reduce-reduce</strong> que teníamos anteriormente, ya que las producciones a reducir en el próximo estado tienen <em>look-ahead</em> distinto. Por completitud, continuemos con el resto del autómata.</p>
<pre><code>I1 = Goto(I0, E) = {
    S -&gt; E., $
}

I2 = Goto(I0, A) = {
    E -&gt; A.= A, $
}

I3 = Goto(I0, i) = {
    E -&gt; i.,    $
    A -&gt; i.+ A, =
    A -&gt; i.,    =
}</code></pre>
<p>Como intuíamos, el estado <span class="math inline"><em>I</em><sub>3</sub></span>, que anteriormente tenía un conflicto <strong>reduce-reduce</strong>, ahora es válido. Veamos el resto de los estados:</p>
<pre><code>I4 = Goto(I2, =) = {
    E -&gt;  A =.A, $
    A -&gt; .i + A, $
    A -&gt; .i,     $
}

I5 = Goto(I3, +) = {
    A -&gt;  i +.A, =
    A -&gt; .i + A, =
    A -&gt; .i,     =
}</code></pre>
<p>Ahora veamos los estados que se derivan de estos. Comenzaremos a notar que aparecen estados muy similares a los anteriores, pero con conjuntos <em>look-ahead</em> distintos:</p>
<pre><code>I6 = Goto(I4, A) = {
    E -&gt; A = A., $
}

I7 = Goto(I4, i) = {
    A -&gt; i.+ A, $
    A -&gt; i.,    $
}

I8 = Goto(I5, A) = {
    A -&gt; i + A., =
}

I9 = Goto(I5, i) = {
    A -&gt; i.+ A, =
    A -&gt; i.,    =
}</code></pre>
<p>Como vemos, <span class="math inline"><em>S</em><sub>7</sub></span> y <span class="math inline"><em>S</em><sub>9</sub></span> son estados en principio idénticos, excepto porque los <em>look-ahead</em> asociados a cada item son distintos. Esta repetición de estados casi iguales será el próximo problema a resolver, pero por el momento continuemos con el autómata:</p>
<pre><code>I10 = Goto(I7, +) = {
    A -&gt;  i +.A, $
    A -&gt; .i + A, $
    A -&gt; .i,     $
}

Goto(I9, +) = S5

I11 = Goto(I10, A) = {
    A -&gt;  i + A., $
}

Goto(I10, i) = S7</code></pre>
<p>El autómata resultante tiene 12 estados. A modo de comparación, el autómata SLR correspondiente tiene 9 estados. Como ya tenemos práctica, lo mostraremos sin más explicación:</p>
<pre><code>I0 = {
    S -&gt; .E
    E -&gt; .A = A
    E -&gt; .i
    A -&gt; .i + A
    A -&gt; .i
}

I1 = Goto(I0, E) = {
    S -&gt; E.
}

I2 = Goto(I0, A) = {
    E -&gt; A.= A
}

I3 = Goto(I0, i) = {
    E -&gt; i.
    A -&gt; i.+ A
    A -&gt; i.
}

I4 = Goto(I2, =) = {
    E -&gt;  A =.A
    A -&gt; .i+ A
    A -&gt; .i
}

I5 = Goto(I3, +) = {
    A -&gt;  i +.A
    A -&gt; .i + A
    A -&gt; .i
}

I6 = Goto(I4, A) = {
    E -&gt; A = A.
}

I7 = Goto(I4, i) = {
    A -&gt; i.+ A
    A -&gt; i.
}

I8 = Goto(I5, A) = {
    A -&gt; i + A.
}

Goto(I5, i) = I7

Goto(I7, +) = I5</code></pre>
<p>Los estados adicionales necesarios en LR son justamente aquellos donde existen conflictos.</p>
<h2 id="parsing-lalr1">Parsing LALR(1)</h2>
<p>Si observamos el autómata LR(1) nuevamente, notaremos algunos estados que son muy semejantes, y solo se diferencian en los conjuntos de <em>look-aheads</em>. En particular, el estado <span class="math inline"><em>I</em><sub>7</sub></span> y el estado <span class="math inline"><em>I</em><sub>9</sub></span> tienen los mismos centros:</p>
<pre><code>I7 = Goto(I4, i) = {
    A -&gt; i.+ A, $
    A -&gt; i.,    $
}

I9 = Goto(I5, i) = {
    A -&gt; i.+ A, =
    A -&gt; i.,    =
}</code></pre>
<p>Al igual que los estados <span class="math inline"><em>I</em><sub>5</sub></span> e <span class="math inline"><em>I</em><sub>10</sub></span>:</p>
<pre><code>I5 = Goto(I3, +) = {
    A -&gt;  i +.A, =
    A -&gt; .i + A, =
    A -&gt; .i,     =
}

I10 = Goto(I7, +) = {
    A -&gt;  i +.A, $
    A -&gt; .i + A, $
    A -&gt; .i,     $
}</code></pre>
<p>Y los estados <span class="math inline"><em>I</em><sub>8</sub></span> e <span class="math inline"><em>I</em><sub>11</sub></span>:</p>
<pre><code>I8 = Goto(I5, A) = {
    A -&gt; i + A., =
}

I11 = Goto(I10, A) = {
    A -&gt; i + A., $
}</code></pre>
<p>Intuitivamente, estos son los estados que ayudan a desambiguar el autómata SLR, pues separan en diferentes subconjuntos de terminales lo que antes era el Follow de un no-terminal. De cierta forma este es el precio a pagar por el poder adicional del autómata LR sobre el SLR: es necesario separar en mútiples estados lo que antes era un solo estado, para poder discriminar con exactitud qué tokens activan una reducción.</p>
<p>Afortunadamente, en ocasiones podemos obtener lo mejor de ambos mundos. ¿Qué sucede si intentamos &quot;compactar&quot; estos estados &quot;duplicados&quot;? Idealmente, si esta compactación no introdujera nuevos conflictos, lograríamos un autómata con menos estados y el mismo poder de reconocimiento. Veamos como podríamos proceder. Tomemos por ejemplo los estados <span class="math inline"><em>I</em><sub>7</sub></span> e <span class="math inline"><em>I</em><sub>9</sub></span> y definamos un nuevo estado <span class="math inline"><em>I</em><sub>7, 9</sub></span>. Para ello simplemente combinamos los <em>look-ahead</em> de cada par de items iguales en un conjunto:</p>
<pre><code>I7-9 = {
    A -&gt; i.+ A, =$
    A -&gt; i.,    =$
}</code></pre>
<p>De la misma forma, podemos hacer con los pares de estados restantes:</p>
<pre><code>I5-10 = {
    A -&gt;  i +.A, =$
    A -&gt; .i + A, =$
    A -&gt; .i,     =$
}

I8-11 = {
    A -&gt; i + A., =$
}</code></pre>
<p>La primera verificación que necesitamos hacer es si estos nuevos estados crean conflictos en sí mismos. Es decir, si al combinar, aparece un conflicto <strong>reduce-reduce</strong> que antes no existía, dado por dos reducciones distintas cuyos <em>look-ahead</em> ahora tengan intersección. En este caso vemos que no sucede, pues en los estados donde hay reducciones, afortunadamente son al mismo no-terminal, por tanto no hay ambigüedad.</p>
<p>Luego tenemos que ver cómo se comporta el autómata si reemplazamos los estados originales por estos nuevos &quot;estados combinados&quot;. Para ello, tenemos que ver que sucede con las transiciones entrantes y salientes. En principio, todas las transiciones que iban a parar a alguno de los estados originales, irán a parar al nuevo estado combinado. Veamos entonces qué aristas entrantes tenía cada estado original:</p>
<pre><code>I7-9  | I7  = Goto(I4, i) = Goto(I10, i)
      | I9  = Goto(I5, i)

I5-10 | I5  = Goto(I3, +) = Goto(I9, +)
      | I10 = Goto(I7, +)

I8-11 | I8  = Goto(I5, A)
      | I11 = Goto(I10, A)</code></pre>
<p>Tenemos entonces que factorizar todas estas aristas entrantes hacia los nuevos estados, y comprobar que no ocurran conflictos. Por ejemplo, como <code>Goto(I4, i) = I7</code>, ahora esa arista irá hacia <code>I7-9</code>. Por otro lado, como <code>Goto(I5, i) = I9</code>, esa arista también irá para <code>I7-9</code>. Y en este segundo caso, como en estado de salida también va a ser parte de un estado combinado, realmente lo que sucederá es que tendremos una arista de <code>I5-10</code> hacia <code>I7-9</code>. Pero para que esto sea posible, necesitamos que la otra arista que salía de <code>I10</code> también caiga en el nuevo estado combinado, pues de lo contrario tendríamos una ambigüedad. Afortunadamente, en este caso <code>Goto(I10, i) = I7</code>, por lo que no existe conflicto. Luego, procedamos a compactar todas las transiciones entrantes:</p>
<pre><code>I7-9  = Goto(I4, i) = Goto(I5-10, i)
I5-10 = Goto(I3, +) = Goto(I7-9, +)
I8-11 = Goto(I5-10, A)</code></pre>
<p>En principio, nos queda ver las aristas salientes de estos estados combinados. En este caso particular todas las aristas salientes ya han sido analizadas, como aristas entrantes de otros estados. En el caso general, es posible que existan aristas salientes de un estado combinado de vayan a parar a estados distintos. Por ejemplo, si hubiera una arista de <span class="math inline"><em>I</em><sub>7</sub></span> con un token <code>c</code> hacia un estado <span class="math inline"><em>I</em><sub><em>i</em></sub></span>, y otra arista desde <span class="math inline"><em>I</em><sub>9</sub></span> con el mismo token <code>c</code> hacia otro estado <span class="math inline"><em>I</em><sub><em>j</em></sub></span>, que no fueran combinables (no tuvieran el mismo centro), entonces no sería posible realizar esta compactación.</p>
<p>Veamos el autómata final que hemos obtenido al combinar los estados con el mismo centro:</p>
<pre><code>I0 = {
    S -&gt; .E,     $
    E -&gt; .A = A, $
    E -&gt; .i,     $
    A -&gt; .i + A, =
    A -&gt; .i,     =
}

I1 = Goto(I0, E) = {
    S -&gt; E., $
}

I2 = Goto(I0, A) = {
    E -&gt; A.= A, $
}

I3 = Goto(I0, i) = {
    E -&gt; i.,    $
    A -&gt; i.+ A, =
    A -&gt; i.,    =
}

I4 = Goto(I2, =) = {
    E -&gt;  A =.A, $
    A -&gt; .i + A, $
    A -&gt; .i,     $
}

I5-10 = Goto(I3, +) = Goto(I7-9, +) {
    A -&gt;  i +.A, =$
    A -&gt; .i + A, =$
    A -&gt; .i,     =$
}

I6 = Goto(I4, A) = {
    E -&gt; A = A., $
}

I7-9 = Goto(I4, i) = Goto(I5-10, i) {
    A -&gt; i.+ A, =$
    A -&gt; i.,    =$
}

I8-11 = Goto(I5-10, A) = {
    A -&gt; i + A., =$
}</code></pre>
<p>El nuevo autómata que hemos construido tiene exactamente 9 estados, la misma cantidad que el autómata SLR, y sin embargo no presenta conflictos. Este autómata se denomina LALR, y constituye el resultado más importante en la práctica para la construcción de compiladores, ya que la mayoría de los generadores de parsers autómaticos usados en la industria construyen este tipo de autómatas. Esto se debe a que combina de forma ideal un poder reconocedor muy similar al LR, con una cantidad de estados mucho menor, proporcional al SLR.</p>
<p>Intuitivamente, el motivo por el que este autómata no presenta conflictos, y el SLR sí, se debe a que en este caso hemos hecho un análisis más riguroso de los Follow primero (construyendo el autómata LR), y solo entonces hemos intentado combinar los estados. Esto fue posible ya que los estados con los mismos centros no eran aquellos donde se producían los conflictos <strong>reduce-reduce</strong> en el autómata SLR. Es decir, al aplicar la técnica LR, y analizar cuidadosamente los Follow, logramos evitar el conflicto en el estado <span class="math inline"><em>I</em><sub>3</sub></span>, pero luego esa misma técnica nos llevó a crear estados independientes innecesarios. De cierta forma, la técnica LR es demasiado rigurosa, y nos lleva incluso a intentar evitar conflictos que en realidad no van a ocurrir. La técnica LALR entonces reconoce estas situaciones donde tenemos &quot;demasiado rigor&quot; y simplifica el autómata tanto como sea posible.</p>
<p>Siguiendo esta línea de pensamiento, debería ser posible construir el autómata LALR directamente, sin necesidad de primero construir el LR para luego combinar los estados innecesarios. De hecho, esto es posible, y es lo que hacen los generadores de parsers usados en la práctica. Aunque no presentaremos un algoritmo detallado para esto, podemos ver intuitivamente que los estados se pueden ir combinando &quot;sobre la marcha&quot;, a medida que se descubren estados con el mismo centro que otros ya creados, y arreglando las transiciones existentes en caso de ser necesario.</p>
<p>Un punto de importancia en el autómata LALR, es que no se puede hacer &quot;una parte&quot; de este. Es decir, o bien todos los estados con el mismo centro de combinan, y no aparece ningún conflicto nuevo, o no se combina ninguno y el autómata se queda LR. De modo que no existen &quot;grados&quot; de LALR.</p>
<h2 id="implementación-del-parser-lr">Implementación del parser LR</h2>
<p>Veamos entonces como construir un algoritmo de parsing lineal que obtenga el árbol de derivación correspondiente. Recordemos que lo que tenemos hasta el momento es un autómata que nos permite reconocer si el contenido de la pila es un prefijo viable. En principio, en cada iteración, tras realizar un <strong>shift</strong> o un <strong>reduce</strong>, es necesario volver a correr el autómata en todo el contenido de la pila, para determinar en qué estado termina, y poder decidir la próxima operación. Esto es innecesariamente costoso. Intuitivamente, dado que tras una operación <strong>shift</strong> o <strong>reduce</strong> sabemos exactamente como cambia la pila, deberíamos poder &quot;hacer backtrack&quot; en el autómata, y solamente ejecutar la parte necesaria para reconocer el nuevo sufijo de la pila.</p>
<p>Por ejemplo, supongamos que tenemos un estado del parser <span class="math inline"><em>α</em>|<em>c</em><em>ω</em></span>, y al ejecutar el autómata, terminamos en un estado <span class="math inline"><em>I</em><sub><em>i</em></sub></span> que indica <strong>shift</strong> con el token <code>c</code>. Si <span class="math inline"><em>G</em><em>o</em><em>t</em><em>o</em>(<em>I</em><sub><em>i</em></sub>, <em>c</em>)=<em>I</em><sub><em>j</em></sub></span>, entonces sabemos que la siguiente iteración terminaremos en el estado <span class="math inline"><em>I</em><sub><em>j</em></sub></span>. Efectivamente, tras un <strong>shift</strong> tendremos un nuevo estado en la pila <span class="math inline"><em>α</em><em>c</em>|<em>ω</em></span>, y como el autómata es determinista, tras reconocer <span class="math inline"><em>α</em></span> tendrá que terminar necesariamente en el estado <span class="math inline"><em>I</em><sub><em>i</em></sub></span>. Luego, el token <code>c</code> lo envía al estado <span class="math inline"><em>I</em><sub><em>j</em></sub></span>, precisamente porque esa es la definición de la función <strong>Goto</strong>. Por tanto, tras una operación de <strong>shift</strong>, no es necesario volver a correr el autómata en todo el contenido de la pila. Simplemente podemos transitar directamente al estado <span class="math inline"><em>G</em><em>o</em><em>t</em><em>o</em>(<em>I</em><sub><em>i</em></sub>, <em>c</em>)</span>.</p>
<p>Veamos que sucede tras una operación <strong>reduce</strong>. Sea <span class="math inline"><em>α</em><em>β</em>|<em>ω</em></span> el estado de la pila antes del <strong>reduce</strong>, y <span class="math inline"><em>α</em><em>X</em>|<em>ω</em></span> el estado después de reducir <span class="math inline"><em>X</em> → <em>β</em></span>. Supongamos que el autómata, tras reconocer <span class="math inline"><em>α</em></span>, cae en el estado <span class="math inline"><em>I</em><sub><em>i</em></sub></span>. Entonces tras reconocer <span class="math inline"><em>α</em><em>X</em></span> debe caer en el estado <span class="math inline"><em>G</em><em>o</em><em>t</em><em>o</em>(<em>I</em><sub><em>i</em></sub>, <em>X</em>)</span> por definición. Por tanto, una vez sacados <span class="math inline">|<em>β</em>|</span> terminales de la pila, solamente necesitamos ser capaces de &quot;recordad&quot; en que estado estaba el autómata cuando reconoció los primeros <span class="math inline">|<em>α</em>|</span> terminales, y de ahí movernos una sola transición. Para ello, sencillamente almacenaremos en la pila, además de la forma oracional que se está construyendo, también los estados que transita el autómata en cada símbolo (ya sea almacenando pares <span class="math inline">&lt;<em>X</em>, <em>i</em>&gt;</span> o con una pila paralela para los estados). Por tanto, cuando extraemos los <span class="math inline">|<em>β</em>|</span> terminales de la pila, en el tope está justamente el estado <span class="math inline"><em>I</em><sub><em>i</em></sub></span>.</p>
<p>Con estas dos estrategias, podemos demostrar que tenemos un algoritmo de parsing lineal. Definamos entonces de una vez y por todas este algoritmo formalmente. Para ello vamos a construir una tabla, que llamaremos <strong>tabla LR</strong>, y que nos indicará en cada situación qué hacer (al estilo de la <strong>tabla LL</strong>). Mostraremos a continuación la <strong>tabla LR</strong> para el autómata construido anteriormente, y luego veremos paso a paso los detalles sobre su construcción:</p>
<table>
<thead>
<tr class="header">
<th align="left">Estado</th>
<th align="left">=</th>
<th align="left">+</th>
<th align="left">i</th>
<th align="left">$</th>
<th align="left">E</th>
<th align="left">A</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">0</td>
<td align="left"></td>
<td align="left"></td>
<td align="left">S3</td>
<td align="left">S-&gt;E</td>
<td align="left">1</td>
<td align="left">2</td>
</tr>
<tr class="even">
<td align="left">1</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left">OK</td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">2</td>
<td align="left">S4</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">3</td>
<td align="left">A-&gt;i</td>
<td align="left"></td>
<td align="left">S5</td>
<td align="left"></td>
<td align="left">E-&gt;i</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">4</td>
<td align="left"></td>
<td align="left"></td>
<td align="left">S7</td>
<td align="left"></td>
<td align="left"></td>
<td align="left">6</td>
</tr>
<tr class="even">
<td align="left">5</td>
<td align="left"></td>
<td align="left"></td>
<td align="left">S9</td>
<td align="left"></td>
<td align="left"></td>
<td align="left">8</td>
</tr>
<tr class="odd">
<td align="left">6</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left">E -&gt; A = A</td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">7</td>
<td align="left"></td>
<td align="left">S10</td>
<td align="left"></td>
<td align="left">A -&gt; i</td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">8</td>
<td align="left">A -&gt; i + A</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">9</td>
<td align="left">A -&gt; i</td>
<td align="left"></td>
<td align="left">S5</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">10</td>
<td align="left"></td>
<td align="left"></td>
<td align="left">S7</td>
<td align="left"></td>
<td align="left"></td>
<td align="left">11</td>
</tr>
<tr class="even">
<td align="left">11</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left">A -&gt; i + A</td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody>
</table>
<p>La tabla LR contiene una fila por cada estado del autómata, y una columna por cada símbolo (terminales y no-terminales). Usaremos la notación <span class="math inline"><em>T</em>[<em>i</em>, <em>x</em>]</span> para referirnos a la entrada asociada al símbolo <span class="math inline"><em>x</em></span> en el estado <span class="math inline"><em>I</em><sub><em>i</em></sub></span>:</p>
<ul>
<li>Si <span class="math inline"><em>S</em> → <em>E</em> ⋅ ,$</span> pertenece al estado <span class="math inline"><em>I</em><sub><em>i</em></sub></span>, entonces la entrada <span class="math inline"><em>T</em>[<em>i</em>, $]=<em>O</em><em>K</em></span>.</li>
<li>Si <span class="math inline"><em>G</em><em>o</em><em>t</em><em>o</em>(<em>I</em><sub><em>i</em></sub>, <em>X</em>)=<em>I</em><sub><em>j</em></sub></span>:t
<ul>
<li>Si <span class="math inline"><em>X</em> ∈ <em>N</em></span>, entonces <span class="math inline"><em>T</em>[<em>i</em>, <em>X</em>]=<em>j</em></span>.</li>
<li>Si <span class="math inline"><em>X</em> ∈ <em>T</em></span>, entonces <span class="math inline"><em>T</em>[<em>i</em>, <em>X</em>]=<em>S</em><sub><em>j</em></sub></span> (<strong>shift</strong>):</li>
</ul></li>
<li>Si el item <span class="math inline"><em>Y</em> → <em>β</em> ⋅ ,<em>c</em></span> está en el estado <span class="math inline"><em>I</em><sub><em>i</em></sub></span>, entonces <span class="math inline"><em>T</em>[<em>i</em>, <em>c</em>]=<em>Y</em> → <em>β</em></span> (<strong>reduce</strong>).</li>
</ul>
<p>Para usar la tabla, veamos qué significa cada posible valor en una entrada. Sea <span class="math inline"><em>S</em> = <em>α</em>|<em>c</em><em>ω</em></span> el estado de la pila de símbolos, e <span class="math inline"><em>I</em><sub><em>i</em></sub></span> el estado del autómata en el tope de la pila de estados (asumiendo una implementación basada en dos pilas paralelas). Buscamos entonces la entrada <span class="math inline"><em>T</em>[<em>i</em>, <em>c</em>]</span>, y según su valor realizamos la siguiente operación:</p>
<ul>
<li>Si <span class="math inline"><em>T</em>[<em>i</em>, <em>c</em>]=<em>O</em><em>K</em></span>, entonces terminamos de parsear y la cadena se reconoce.</li>
<li>Si <span class="math inline"><em>T</em>[<em>i</em>, <em>c</em>]=<em>S</em><sub><em>j</em></sub></span>, entonces hacemos <strong>shift</strong>, la pila de símbolos se convierte en <span class="math inline"><em>α</em><em>c</em>|<em>ω</em></span>, y ponemos el estado <span class="math inline"><em>I</em><sub><em>j</em></sub></span> en el tope de la pila de estados.</li>
<li>Si <span class="math inline"><em>T</em>[<em>i</em>, <em>c</em>]=<em>X</em> → <em>δ</em></span>, entonces se garantiza que <span class="math inline"><em>α</em> = <em>β</em><em>δ</em></span>. Extraemos <span class="math inline">|<em>δ</em>|</span> elementos de la pila de símbolos <strong>y</strong> de la pila de estados. Sea <span class="math inline"><em>I</em><sub><em>k</em></sub></span> el estado que queda en el tope de la pila de estados; colocamos a <span class="math inline"><em>X</em></span> en el tope de la símbolos y colocamos <span class="math inline"><em>T</em>[<em>k</em>, <em>X</em>]</span> en el tope de la pila de estados.</li>
<li>En cualquier otro caso, se ha encontrado un error.</li>
</ul>
<p>Para entender por qué el algoritmo descrito anteriormente funciona, tratemos de interpretar qué significa cada entrada en la tabla. Una entrada de la forma <span class="math inline"><em>S</em><sub><em>j</em></sub></span> significa que con el token correspondiente existe una transición hacia el estado <span class="math inline"><em>I</em><sub><em>j</em></sub></span>. Por tanto, la operación a realizar es <strong>shift</strong>, y como ya hemos visto anteriormente, en la pila de estados sabemos que el siguiente estado hacia el que transitar será justamente <span class="math inline"><em>j</em></span>. Una entrada de la forma <span class="math inline"><em>X</em> → <em>β</em></span> significa que existe un item <strong>reduce</strong> con el token correspondiente de <em>look-ahead</em>. Por lo tanto se realiza la operación de <strong>reduce</strong>. Como vimos anteriormente, en el tope de la pila de estados quedará el estado <span class="math inline"><em>I</em><sub><em>j</em></sub></span> que habíamos transitado justo antes de reconocer la producción reducida. Por tanto, de ese estado anterior, el nuevo estado al que transitaremos es justamente <span class="math inline"><em>T</em>[<em>j</em>, <em>X</em>]</span>.</p>
<p>El único punto que nos queda por discutir, es cómo se construye el árbol de derivación. Esbozaremos un algoritmo para esto, que ejemplificaremos más adelante. Dado que la derivación se construye de abajo hacia arriba, intuitivamente puede verse que construiremos el árbol empezando por las hojas. La idea general consiste en mantener una tercera pila donde se irán acumulando sub-árboles de derivación. Cada vez que se haga una operación <strong>reduce</strong> <span class="math inline"><em>X</em> → <em>β</em></span>, tendremos en el tope de esta pila <span class="math inline">|<em>β</em></span> sub-árboles de derivación, que agruparemos bajo una nueva raíz <span class="math inline"><em>X</em></span>. Al finalizar el reconocimiento, en la pila quedará solamente un árbol, que será la derivación de toda la cadena.</p>
<p>Veamos ahora un ejemplo de cómo funciona el algoritmo de parsing bottom-up con la cadena <code>i = i + i</code>, usando la tabla definida anteriormente. Ilustraremos la ejecución del algoritmo, representando el estado de la pila de símbolos de la forma usual, y además representando las dos pilas correspondientes a los estados y los sub-árboles de derivación. Para representar un árbol, usaremos la notación <span class="math inline"><em>X</em>(<em>t</em><sub>1</sub>, <em>t</em><sub>2</sub>, …, <em>t</em><sub><em>n</em></sub>)</span>, donde <span class="math inline"><em>X</em></span> es el símbolo de la raíz, y <span class="math inline"><em>t</em><sub><em>i</em></sub></span> es una hoja, o un árbol a su vez. Comenzamos entonces con todas las pilas en su estado inicial:</p>
<pre><code>Symbols ::  |i = i + i $
States  :: 0|
Trees   ::  |</code></pre>
<p>Consultamos la tabla. En el estado 0, con <em>look-ahead</em> <code>i</code>, la operación es <strong>shift 3</strong>. Notemos cómo en este estado inicial, todos los demás token darían error. Colocamos el terminal, el estado, y el árbol recién creado en el tope de las respectivas pilas:</p>
<pre><code>Symbols ::   i|= i + i $
States  :: 0 3|
Trees   ::   i|</code></pre>
<p>Nos encontramos ahora en el estado 3, con <em>look-ahead</em> <code>=</code>, la operación es <strong>reduce</strong> <code>A -&gt; i</code>. Entonces sacamos el token <code>i</code> de la pila y lo reemplazamos por <code>A</code>. El estado 3 también se saca y se reemplaza por <span class="math inline"><em>G</em><em>o</em><em>t</em><em>o</em>(0, <em>A</em>)=2</span> (0 es el estado que queda justo debajo de 3 en la pila). El árbol <span class="math inline"><em>i</em></span> se ubica como único hijo del nuevo árbol <span class="math inline"><em>A</em></span> creado:</p>
<pre><code>Symbols ::    A|= i + i $
States  ::  0 2|
Trees   :: A(i)|</code></pre>
<p>Consultamos de nuevo la tabla, <span class="math inline"><em>T</em>[2, =]</span> es <strong>shift 4</strong>:</p>
<pre><code>Symbols ::    A =|i + i $
States  ::  0 2 4|
Trees   :: A(i) =|</code></pre>
<p>Consultamos de nuevo, <span class="math inline"><em>T</em>[4, <em>i</em>]</span> es <strong>shift 7</strong>. En la pila de árboles se nos han ido acumulando sub-árboles distintos, que serán mezclados en el futuro:</p>
<pre><code>Symbols ::    A = i|+ i $
States  ::  0 2 4 7|
Trees   :: A(i) = i|</code></pre>
<p>Ahora <span class="math inline"><em>T</em>[7, +]</span> nos dice <strong>shift 10</strong>:</p>
<pre><code>Symbols ::    A = i +|i $
States  :: 0 2 4 7 10|
Trees   :: A(i) = i +|</code></pre>
<p>Por último, <span class="math inline"><em>T</em>[10, <em>i</em>]</span> nos dice nuevamente <strong>shift 7</strong>:</p>
<pre><code>Symbols ::    A = i + i|$
States  :: 0 2 4 7 10 7|
Trees   :: A(i) = i + i|</code></pre>
<p>Volvemos entonces al estado <span class="math inline"><em>I</em><sub>7</sub></span>, pero ahora con <em>look-ahead</em> <code>$</code> la operación indicada es <strong>reduce</strong> <code>A -&gt; i</code>. El nuevo estado será <span class="math inline"><em>G</em><em>o</em><em>t</em><em>o</em>(10, <em>A</em>)=11</span>.</p>
<pre><code>Symbols ::       A = i + A|$
States  ::   0 2 4 7 10 11|
Trees   :: A(i) = i + A(i)|</code></pre>
<p>Ahora <span class="math inline"><em>T</em>[11,</span>]$ también nos indica <strong>reduce</strong> pero en este caso en <code>A -&gt; i + A</code>. Vamos a sacar entonces 3 elementos de cada pila. En la pila de símbolos, sustituímos <code>i + A</code> por <code>A</code>. En la pila de estados, sacamos los tres últimos elementos, y ponemos <span class="math inline"><em>G</em><em>o</em><em>t</em><em>o</em>(4, <em>A</em>)=6</span>. En la pila de árboles, sacamos los tres árboles del tope y creamos un nuevo árbol <span class="math inline"><em>A</em></span> con esos tres como hijos:</p>
<pre><code>Symbols ::              A = A|$
States  ::            0 2 4 6|
Trees   :: A(i) = A(i,+,A(i))|</code></pre>
<p>Hemos dado un gran salto de fé, pues al sacar los últimos tres estados, hemos confiado en que la pila de estados nos recordará dónde estaba el autómata justo antes de hacer el primer <strong>shift</strong> que dio paso a toda la forma oracional <code>i + A</code>. Continuemos entonces con <span class="math inline"><em>T</em>[6, $]</span> que nos dice justo lo que esperábamos: <strong>reduce</strong> <code>E -&gt; A = A</code>. Repetimos toda la operación de reducción que ya conocemos, siendo <span class="math inline"><em>G</em><em>o</em><em>t</em><em>o</em>(0, <em>E</em>)=1</span> el último estado que tendremos que analizar:</p>
<pre><code>Symbols ::                     E|$
States  ::                   0 1|
Trees   :: E(A(i),=,A(i,+,A(i)))|</code></pre>
<p>Finalmente, <span class="math inline"><em>T</em>[1, <em>E</em>]</span> nos dice que la cadena ha sido parseada. En el tope de la pila de símbolos queda el símbolo inicial, y en la pila de árboles hay un solo árbol que contiene la derivación que hemos construido. Honestamente, en una implementación computacional concreta la pila de símbolos es innecesaria, pues todas las decisiones se toman mirando solamente la pila de estados, y el resultado necesario se computa en la pila de árboles. De cierta forma, esta pila de árboles es un <em>upgrade</em> de la pila de símbolos. Hemos hecho la distinción en este ejemplo por cuestiones puramente didácticas, pero en la práctica, la pila de símbolos no se usa.</p>
<h2 id="comparaciones-entre-ll-slr-lr-y-lalr">Comparaciones entre LL, SLR, LR y LALR</h2>
<p>Hemos visto hasta el momento los dos paradigmas fundamentales para el parsing determinista de gramáticas libres del contexto. Hagamos entonces una reflexión final sobre los resultados que hemos obtenido, y las herramientas que hemos desarrollado.</p>
<p>El parser LL es posiblemente el más sencillo de todos los parsers que es útil. En muchos contextos donde se tiene que diseñar un lenguaje bien sencillo (por ejemplo, un DSL integrado en un sistema más complejo), este parser puede brindar una solución fácil y eficiente. La mayor ventaja es que se puede escribir directamente, sin necesidad de usar un generador de parser. Los conjuntos First y Follow se computan a mano, y luego se escriben cada uno de los métodos recursivos asociados a las producciones. Para cualquier lenguaje de complejidad algo mayor, recurriremos entonces a construir un parser LR.</p>
<p>De manera general, el autómata SLR tiene una cantidad considerablemente menor de estados que el autómata LR correspondiente. En los casos donde la gramática es SLR(1) es preferible usar dicho autómata. Desgraciadamente, la mayoría de las construcciones sintácticas de interés para los lenguajes de programación usuales tienen gramáticas &quot;naturales&quot; que no son SLR, pero sí LR, y generalmente LALR. Por este motivo, el parser LR(1) es, en la práctica, el más usado. Su mayor desventaja radica en el elevado número de estados, que dificultan su almacenamiento. Este problema era especialmente complejo en el año 1965, cuando Donald Knuth propuso este parser. Por tal motivo, aunque teóricamente es una solución adecuada, en la práctica no se usó hasta que en 1969 James DeRemer propuso el parser LALR, que reduce considerablemente la cantidad de estados hasta un nivel comparable con el SLR, sin perder prácticamente en expresividad con respecto con al LR. Más adelante en 1971 el propio DeRemer propondría el SLR como una variante más sencilla de construir algo parecido al LALR de forma más sencilla.</p>
<p>De modo que, en una situación real, la primera decisión podría ser intentar directamente construir un parser LALR. Si esto funciona, no hay nada más que hacer. En caso contrario, deberíamos probar entonces a construir un parser LR, aunque tenga una cantidad mucho mayor de estados. En el caso peor en que esto no sea posible, tendremos que modificar la gramática. En la práctica esto no sucede comúnmente. Aunque es cierto que es fácil encontrar lenguajes didácticos que no sean LR y sean bien pequeños, los lenguajes de programación reales tienen construcciones que generalmente sí son LR. Incluso en los casos en que esto no sucede, veremos más adelante que es posible &quot;pasar&quot; algunos de los problemas del lenguaje para la fase semántica, y simplificar la gramática, haciéndola LR.</p>
<p>Desde el punto de vista teórico, tenemos una jerarquía de gramáticas que se comporta de la siguiente forma:</p>
<pre><code>+------------------+
|        LR        |
|     +------+     |
|     |  LL  |     |
| +---|------|---+ |
| |   | LALR |   | |
| | +-|------|-+ | |
| | | | SLR  | | | |
| | +-|------|-+ | |
| +---|------|---+ |
|     +------+     |
+------------------+</code></pre>
<p>Es decir, las gramáticas LR son un conjunto estrictamente superior a las gramáticas LL, SLR y LALR. Entre las gramáticas SLR, LALR y LR hay una relación de inclusión que es un orden total. Sin embargo, aunque las gramáticas LL están estrictamente incluidas en las LR, existen gramáticas LL que no son ni SLR ni LALR.</p>
<p>Desde el punto de vista de la implementación, los parsers SLR, LALR y LR son idénticos. Solamente se diferencian en la forma en que se construye el autómata, que en última instancia determina la tabla obtenida. De la tabla en adelante el algoritmo de parsing es el mismo que hemos visto. De modo que, en una implementación concreta, es posible desacoplar el mecanismo que genera la tabla, del mecanismo que la ejecuta, y reutilizar toda la segunda parte para cualquiera de los tres tipos de parsers.</p>
<p>Otra cuestión interesante es qué sucede con los errores de parsing. Knuth demostró que el autómata LR reconoce los errores de parsing lo antes posible, para cualquier clase de parser determinista. Esto quiere decir que con la misma cadena (incorrecta) de entrada, ningún algoritmo de parsing podrá darse cuenta de que la cadena es errónea antes que el algoritmo LR. De hecho, los parsers LALR y SLR son más permisivos. Dada una cadena incorrecta, es posible que el parser SLR o LALR haga algunas reducciones de más, antes de darse cuenta de que la cadena es inválida. Al final ninguno de estos parsers deja pasar incorrectamente una cadena inválida, por supuesto. Pero reconocer un error lo antes posible, además de la ventaja en eficiencia, es también muy conveniente para brindar al programar un mensaje de error lo más acertado posible.</p>
<p>Cabe preguntarse entonces si al inventar el autómata LR hemos definitivamente terminado con el problema de parsing. Pues resulta que la respuesta teórica para esta pregunta es <strong>sí</strong>. En 1965 Knuth demostró que para todo lenguaje libre del contexto determinista tiene que existir una gramática LR(k). Los lenguajes libres del contexto deterministas son aquellos tales que existe un algoritmo de parsing lineal en la longitud de la cadena en caso peor. Por otro lado, toda gramática LR(k) puede ser convertida a LR(1), con la adición de nuevos no-terminales y producciones. De modo que tenemos un resultado teórico que dice: si un lenguaje puede ser parseado en tiempo lineal, entonces puede ser parseado con un parser LR(1). Aquellos lenguajes libres del contexto que no son LR(1) tienen que ser por necesidad ambiguos, o al menos es imposible diseñar un algoritmo de parsing con tiempo lineal. De cierta forma, podemos decir que hemos terminado, pues todo lenguaje &quot;sensato&quot; es LR(1).</p>
<p>Este resultado es de hecho impresionante, pero la historia no acaba ahí. Incluso aunque teóricamente LR(1) es suficiente, en la práctica hay lenguajes deterministas cuyas gramáticas LR(1) son tan complicadas, que es preferible usar una gramática más flexible, incluso incluyendo algo de ambiguedad que pueda ser resuelto en una fase superior. Afortunadamente, el problema de encontrar un árbol de derivación para cualquier gramática libre del contexto tiene solución con caso peor <span class="math inline"><em>O</em>(<em>n</em><sup>3</sup>)</span> con respecto a la longitud de la cadena. Es decir, existen parsers generales que pueden reconocer cualquier lenguaje libre del contexto, <em>incluso lenguajes ambiguos</em>, y en estos casos se pueden obtener <strong>todos</strong> los árboles de derivación que existen. En la práctica sin embargo, para diseñar lenguajes de programación, queremos parsers lineales por motivos de eficiencia. Estos parsers más generales se emplean sobre todo en tareas de procesamiento de lenguaje natural (p.e. traducción automática).</p>
<nav>
    <a href="chap2.html" class="navigation previous">Anterior</a>
      <a href="chap4.html" class="navigation next">Siguiente</a>
  </nav>
</body>
</html>
